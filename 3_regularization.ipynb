{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST_200k.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune L2 regularization for logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  regularizers = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases) \n",
    "  # Add the regularization term to the loss.\n",
    "  loss += 5e-4 * regularizers\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 19.274778\n",
      "Minibatch loss at step 100: 3.159260\n",
      "Minibatch loss at step 200: 3.174854\n",
      "Minibatch loss at step 300: 2.899036\n",
      "Minibatch loss at step 400: 2.838298\n",
      "Minibatch loss at step 500: 2.099874\n",
      "Minibatch loss at step 600: 2.393768\n",
      "Minibatch loss at step 700: 2.265441\n",
      "Minibatch loss at step 800: 1.670291\n",
      "Minibatch loss at step 900: 1.835260\n",
      "Minibatch loss at step 1000: 1.813816\n",
      "Minibatch loss at step 1100: 1.802212\n",
      "Minibatch loss at step 1200: 1.460770\n",
      "Minibatch loss at step 1300: 1.606140\n",
      "Minibatch loss at step 1400: 1.346922\n",
      "Minibatch loss at step 1500: 1.028840\n",
      "Minibatch loss at step 1600: 1.411310\n",
      "Minibatch loss at step 1700: 1.183965\n",
      "Minibatch loss at step 1800: 1.403946\n",
      "Minibatch loss at step 1900: 1.379503\n",
      "Minibatch loss at step 2000: 0.977990\n",
      "Minibatch loss at step 2100: 1.030939\n",
      "Minibatch loss at step 2200: 1.168305\n",
      "Minibatch loss at step 2300: 1.238066\n",
      "Minibatch loss at step 2400: 0.896950\n",
      "Minibatch loss at step 2500: 1.008323\n",
      "Minibatch loss at step 2600: 1.005326\n",
      "Minibatch loss at step 2700: 0.916784\n",
      "Minibatch loss at step 2800: 1.005784\n",
      "Minibatch loss at step 2900: 0.745746\n",
      "Minibatch loss at step 3000: 0.915378\n",
      "Minibatch loss at step 3100: 0.885735\n",
      "Minibatch loss at step 3200: 0.731568\n",
      "Minibatch loss at step 3300: 0.710216\n",
      "Minibatch loss at step 3400: 0.769403\n",
      "Minibatch loss at step 3500: 0.830999\n",
      "Minibatch loss at step 3600: 1.007215\n",
      "Minibatch loss at step 3700: 0.968214\n",
      "Minibatch loss at step 3800: 0.618396\n",
      "Minibatch loss at step 3900: 0.916281\n",
      "Minibatch loss at step 4000: 0.764060\n",
      "Minibatch loss at step 4100: 0.792180\n",
      "Minibatch loss at step 4200: 0.653359\n",
      "Minibatch loss at step 4300: 0.653790\n",
      "Minibatch loss at step 4400: 0.595844\n",
      "Minibatch loss at step 4500: 0.712415\n",
      "Minibatch loss at step 4600: 0.865167\n",
      "Minibatch loss at step 4700: 0.884890\n",
      "Minibatch loss at step 4800: 0.688358\n",
      "Minibatch loss at step 4900: 0.658503\n",
      "Minibatch loss at step 5000: 0.636786\n",
      "Minibatch loss at step 5100: 0.720673\n",
      "Minibatch loss at step 5200: 0.658217\n",
      "Minibatch loss at step 5300: 0.630202\n",
      "Minibatch loss at step 5400: 0.660083\n",
      "Minibatch loss at step 5500: 0.799874\n",
      "Minibatch loss at step 5600: 0.538995\n",
      "Minibatch loss at step 5700: 0.739732\n",
      "Minibatch loss at step 5800: 0.475315\n",
      "Minibatch loss at step 5900: 0.872634\n",
      "Minibatch loss at step 6000: 0.838416\n",
      "Minibatch loss at step 6100: 0.784072\n",
      "Minibatch loss at step 6200: 0.570525\n",
      "Minibatch loss at step 6300: 0.812999\n",
      "Minibatch loss at step 6400: 0.703991\n",
      "Minibatch loss at step 6500: 0.552981\n",
      "Minibatch loss at step 6600: 0.622935\n",
      "Minibatch loss at step 6700: 0.674779\n",
      "Minibatch loss at step 6800: 0.790641\n",
      "Minibatch loss at step 6900: 0.780472\n",
      "Minibatch loss at step 7000: 0.699618\n",
      "Minibatch loss at step 7100: 0.790731\n",
      "Minibatch loss at step 7200: 0.971540\n",
      "Minibatch loss at step 7300: 0.739623\n",
      "Minibatch loss at step 7400: 0.614385\n",
      "Minibatch loss at step 7500: 0.920658\n",
      "Minibatch loss at step 7600: 0.612395\n",
      "Minibatch loss at step 7700: 0.741081\n",
      "Minibatch loss at step 7800: 0.545510\n",
      "Minibatch loss at step 7900: 0.639532\n",
      "Minibatch loss at step 8000: 1.044680\n",
      "Minibatch loss at step 8100: 0.645037\n",
      "Minibatch loss at step 8200: 0.547478\n",
      "Minibatch loss at step 8300: 0.867060\n",
      "Minibatch loss at step 8400: 0.585583\n",
      "Minibatch loss at step 8500: 0.588289\n",
      "Minibatch loss at step 8600: 0.668644\n",
      "Minibatch loss at step 8700: 0.633364\n",
      "Minibatch loss at step 8800: 0.742259\n",
      "Minibatch loss at step 8900: 0.586623\n",
      "Minibatch loss at step 9000: 0.755317\n",
      "Minibatch loss at step 9100: 0.543166\n",
      "Minibatch loss at step 9200: 0.734314\n",
      "Minibatch loss at step 9300: 0.778404\n",
      "Minibatch loss at step 9400: 0.435244\n",
      "Minibatch loss at step 9500: 0.603045\n",
      "Minibatch loss at step 9600: 0.663355\n",
      "Minibatch loss at step 9700: 0.839823\n",
      "Minibatch loss at step 9800: 0.607102\n",
      "Minibatch loss at step 9900: 0.786876\n",
      "Minibatch loss at step 10000: 0.788234\n",
      "Minibatch loss at step 10100: 0.806140\n",
      "Minibatch loss at step 10200: 0.891206\n",
      "Minibatch loss at step 10300: 0.672095\n",
      "Minibatch loss at step 10400: 0.713866\n",
      "Minibatch loss at step 10500: 0.655709\n",
      "Minibatch loss at step 10600: 0.646173\n",
      "Minibatch loss at step 10700: 0.825690\n",
      "Minibatch loss at step 10800: 0.664745\n",
      "Minibatch loss at step 10900: 0.627215\n",
      "Minibatch loss at step 11000: 0.717847\n",
      "Minibatch loss at step 11100: 0.645502\n",
      "Minibatch loss at step 11200: 0.588898\n",
      "Minibatch loss at step 11300: 0.511535\n",
      "Minibatch loss at step 11400: 0.661487\n",
      "Minibatch loss at step 11500: 0.783481\n",
      "Minibatch loss at step 11600: 0.634261\n",
      "Minibatch loss at step 11700: 0.568155\n",
      "Minibatch loss at step 11800: 0.672224\n",
      "Minibatch loss at step 11900: 0.622444\n",
      "Minibatch loss at step 12000: 0.827746\n",
      "Minibatch loss at step 12100: 0.723913\n",
      "Minibatch loss at step 12200: 0.504314\n",
      "Minibatch loss at step 12300: 0.781969\n",
      "Minibatch loss at step 12400: 0.510213\n",
      "Minibatch loss at step 12500: 0.676826\n",
      "Minibatch loss at step 12600: 0.839787\n",
      "Minibatch loss at step 12700: 0.733906\n",
      "Minibatch loss at step 12800: 0.624014\n",
      "Minibatch loss at step 12900: 0.740021\n",
      "Minibatch loss at step 13000: 0.767927\n",
      "Minibatch loss at step 13100: 0.855968\n",
      "Minibatch loss at step 13200: 0.570423\n",
      "Minibatch loss at step 13300: 0.812095\n",
      "Minibatch loss at step 13400: 0.476084\n",
      "Minibatch loss at step 13500: 0.724792\n",
      "Minibatch loss at step 13600: 0.664322\n",
      "Minibatch loss at step 13700: 0.700927\n",
      "Minibatch loss at step 13800: 0.772183\n",
      "Minibatch loss at step 13900: 0.658602\n",
      "Minibatch loss at step 14000: 0.614626\n",
      "Minibatch loss at step 14100: 0.596928\n",
      "Minibatch loss at step 14200: 0.730791\n",
      "Minibatch loss at step 14300: 0.500886\n",
      "Minibatch loss at step 14400: 0.661930\n",
      "Minibatch loss at step 14500: 0.858975\n",
      "Minibatch loss at step 14600: 0.616095\n",
      "Minibatch loss at step 14700: 0.716496\n",
      "Minibatch loss at step 14800: 0.704666\n",
      "Minibatch loss at step 14900: 0.854668\n",
      "Minibatch loss at step 15000: 0.692850\n",
      "Minibatch loss at step 15100: 0.731708\n",
      "Minibatch loss at step 15200: 0.786339\n",
      "Minibatch loss at step 15300: 0.822433\n",
      "Minibatch loss at step 15400: 0.841261\n",
      "Minibatch loss at step 15500: 0.752311\n",
      "Minibatch loss at step 15600: 0.539074\n",
      "Minibatch loss at step 15700: 0.570590\n",
      "Minibatch loss at step 15800: 0.554768\n",
      "Minibatch loss at step 15900: 0.613613\n",
      "Minibatch loss at step 16000: 0.561731\n",
      "Minibatch loss at step 16100: 0.511641\n",
      "Minibatch loss at step 16200: 0.612129\n",
      "Minibatch loss at step 16300: 0.745758\n",
      "Minibatch loss at step 16400: 0.672603\n",
      "Minibatch loss at step 16500: 0.608443\n",
      "Minibatch loss at step 16600: 0.952955\n",
      "Minibatch loss at step 16700: 0.837059\n",
      "Minibatch loss at step 16800: 0.785441\n",
      "Minibatch loss at step 16900: 0.551556\n",
      "Minibatch loss at step 17000: 0.655961\n",
      "Minibatch loss at step 17100: 0.553962\n",
      "Minibatch loss at step 17200: 0.549369\n",
      "Minibatch loss at step 17300: 0.779756\n",
      "Minibatch loss at step 17400: 0.598028\n",
      "Minibatch loss at step 17500: 0.357150\n",
      "Minibatch loss at step 17600: 0.636571\n",
      "Minibatch loss at step 17700: 0.637390\n",
      "Minibatch loss at step 17800: 0.952135\n",
      "Minibatch loss at step 17900: 0.612217\n",
      "Minibatch loss at step 18000: 0.498587\n",
      "Minibatch loss at step 18100: 0.504968\n",
      "Minibatch loss at step 18200: 0.960458\n",
      "Minibatch loss at step 18300: 0.507675\n",
      "Minibatch loss at step 18400: 0.720638\n",
      "Minibatch loss at step 18500: 0.656866\n",
      "Minibatch loss at step 18600: 0.792834\n",
      "Minibatch loss at step 18700: 0.924502\n",
      "Minibatch loss at step 18800: 0.643296\n",
      "Minibatch loss at step 18900: 0.528865\n",
      "Minibatch loss at step 19000: 0.455499\n",
      "Minibatch loss at step 19100: 0.662108\n",
      "Minibatch loss at step 19200: 0.829622\n",
      "Minibatch loss at step 19300: 0.680001\n",
      "Minibatch loss at step 19400: 0.771380\n",
      "Minibatch loss at step 19500: 0.552244\n",
      "Minibatch loss at step 19600: 0.706429\n",
      "Minibatch loss at step 19700: 0.651922\n",
      "Minibatch loss at step 19800: 0.487162\n",
      "Minibatch loss at step 19900: 0.736282\n",
      "Minibatch loss at step 20000: 0.803365\n",
      "Minibatch loss at step 20100: 0.826138\n",
      "Minibatch loss at step 20200: 0.839182\n",
      "Minibatch loss at step 20300: 0.683304\n",
      "Minibatch loss at step 20400: 0.816770\n",
      "Minibatch loss at step 20500: 0.644876\n",
      "Minibatch loss at step 20600: 0.616621\n",
      "Minibatch loss at step 20700: 0.536625\n",
      "Minibatch loss at step 20800: 0.642302\n",
      "Minibatch loss at step 20900: 0.815978\n",
      "Minibatch loss at step 21000: 0.650161\n",
      "Minibatch loss at step 21100: 0.736647\n",
      "Minibatch loss at step 21200: 0.689010\n",
      "Minibatch loss at step 21300: 0.854532\n",
      "Minibatch loss at step 21400: 0.608437\n",
      "Minibatch loss at step 21500: 0.551018\n",
      "Minibatch loss at step 21600: 0.750900\n",
      "Minibatch loss at step 21700: 0.490575\n",
      "Minibatch loss at step 21800: 0.654922\n",
      "Minibatch loss at step 21900: 0.692427\n",
      "Minibatch loss at step 22000: 0.473287\n",
      "Minibatch loss at step 22100: 0.775836\n",
      "Minibatch loss at step 22200: 0.702355\n",
      "Minibatch loss at step 22300: 0.561147\n",
      "Minibatch loss at step 22400: 0.737490\n",
      "Minibatch loss at step 22500: 0.588885\n",
      "Minibatch loss at step 22600: 0.780574\n",
      "Minibatch loss at step 22700: 0.627135\n",
      "Minibatch loss at step 22800: 0.802833\n",
      "Minibatch loss at step 22900: 0.625855\n",
      "Minibatch loss at step 23000: 0.655736\n",
      "Minibatch loss at step 23100: 0.719095\n",
      "Minibatch loss at step 23200: 0.589231\n",
      "Minibatch loss at step 23300: 0.739644\n",
      "Minibatch loss at step 23400: 0.724684\n",
      "Minibatch loss at step 23500: 0.565288\n",
      "Minibatch loss at step 23600: 0.688063\n",
      "Minibatch loss at step 23700: 0.602609\n",
      "Minibatch loss at step 23800: 0.696334\n",
      "Minibatch loss at step 23900: 1.045790\n",
      "Minibatch loss at step 24000: 0.632150\n",
      "Minibatch loss at step 24100: 0.736332\n",
      "Minibatch loss at step 24200: 0.783376\n",
      "Minibatch loss at step 24300: 0.819360\n",
      "Minibatch loss at step 24400: 0.512830\n",
      "Minibatch loss at step 24500: 0.691353\n",
      "Minibatch loss at step 24600: 0.605393\n",
      "Minibatch loss at step 24700: 0.697420\n",
      "Minibatch loss at step 24800: 0.793575\n",
      "Minibatch loss at step 24900: 0.598471\n",
      "Minibatch loss at step 25000: 0.790312\n",
      "Minibatch loss at step 25100: 0.768260\n",
      "Minibatch loss at step 25200: 0.555722\n",
      "Minibatch loss at step 25300: 0.520966\n",
      "Minibatch loss at step 25400: 0.721533\n",
      "Minibatch loss at step 25500: 0.719222\n",
      "Minibatch loss at step 25600: 0.629148\n",
      "Minibatch loss at step 25700: 0.655751\n",
      "Minibatch loss at step 25800: 0.496374\n",
      "Minibatch loss at step 25900: 0.449265\n",
      "Minibatch loss at step 26000: 0.758320\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2da20b9d8b9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0mtrainAcc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m       \u001b[0mvalidAcc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test accuracy: %.1f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \"\"\"\n\u001b[0;32m--> 460\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   2908\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2909\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 2910\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 428\u001b[0;31m                                target_list)\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 30001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  lossVec = []\n",
    "  trainAcc = []\n",
    "  validAcc = []\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    lossVec.append(l)\n",
    "    if (step % 100 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      #print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      #print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "      \n",
    "      trainAcc.append(accuracy(predictions, batch_labels))\n",
    "      validAcc.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.clf()\n",
    "plt.plot(lossVec)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xrange(0,3001,100),trainAcc, label= \"Minibatch Accuracy\")\n",
    "plt.plot(xrange(0,3001,100),validAcc, label= \"Valid Accuracy\")\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune L2 regularization for neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weightsHidden = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_size]))\n",
    "  biasesHidden = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "    \n",
    "  weights = tf.Variable(tf.truncated_normal([hidden_layer_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  logitsHidden = tf.matmul(tf_train_dataset, weightsHidden) + biasesHidden\n",
    "  hiddenLayer = tf.nn.relu(logitsHidden)\n",
    "\n",
    "  # Training computation.\n",
    "  logits = tf.matmul(hiddenLayer, weights) + biases\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  regularizers = (tf.nn.l2_loss(weightsHidden) + tf.nn.l2_loss(biasesHidden) +\n",
    "                  tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases))\n",
    "  # Add the regularization term to the loss.\n",
    "  loss += 5e-4 * regularizers\n",
    " \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "  valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weightsHidden) + biasesHidden)\n",
    "  valid_logits = tf.matmul(valid_hidden, weights) + biases\n",
    "  valid_prediction = tf.nn.softmax(valid_logits)\n",
    "    \n",
    "  test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weightsHidden) + biasesHidden)\n",
    "  test_logits = tf.matmul(test_hidden, weights) + biases\n",
    "  test_prediction = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 492.566711\n",
      "Minibatch loss at step 100: 164.932800\n",
      "Minibatch loss at step 200: 179.862656\n",
      "Minibatch loss at step 300: 152.022263\n",
      "Minibatch loss at step 400: 146.356796\n",
      "Minibatch loss at step 500: 133.668655\n",
      "Minibatch loss at step 600: 121.251968\n",
      "Minibatch loss at step 700: 130.789398\n",
      "Minibatch loss at step 800: 107.102165\n",
      "Minibatch loss at step 900: 101.696976\n",
      "Minibatch loss at step 1000: 100.344101\n",
      "Minibatch loss at step 1100: 91.339127\n",
      "Minibatch loss at step 1200: 86.512970\n",
      "Minibatch loss at step 1300: 82.582367\n",
      "Minibatch loss at step 1400: 77.761520\n",
      "Minibatch loss at step 1500: 74.231133\n",
      "Minibatch loss at step 1600: 70.891693\n",
      "Minibatch loss at step 1700: 65.501678\n",
      "Minibatch loss at step 1800: 63.426483\n",
      "Minibatch loss at step 1900: 62.126720\n",
      "Minibatch loss at step 2000: 57.097679\n",
      "Minibatch loss at step 2100: 53.752029\n",
      "Minibatch loss at step 2200: 52.936195\n",
      "Minibatch loss at step 2300: 50.468494\n",
      "Minibatch loss at step 2400: 47.995010\n",
      "Minibatch loss at step 2500: 45.121170\n",
      "Minibatch loss at step 2600: 45.541100\n",
      "Minibatch loss at step 2700: 39.998589\n",
      "Minibatch loss at step 2800: 38.997334\n",
      "Minibatch loss at step 2900: 36.343548\n",
      "Minibatch loss at step 3000: 34.354286\n",
      "Minibatch loss at step 3100: 33.215092\n",
      "Minibatch loss at step 3200: 31.034819\n",
      "Minibatch loss at step 3300: 29.585690\n",
      "Minibatch loss at step 3400: 28.234467\n",
      "Minibatch loss at step 3500: 26.927034\n",
      "Minibatch loss at step 3600: 25.715473\n",
      "Minibatch loss at step 3700: 24.589975\n",
      "Minibatch loss at step 3800: 23.161589\n",
      "Minibatch loss at step 3900: 21.888363\n",
      "Minibatch loss at step 4000: 20.953030\n",
      "Minibatch loss at step 4100: 19.790009\n",
      "Minibatch loss at step 4200: 19.114216\n",
      "Minibatch loss at step 4300: 17.907106\n",
      "Minibatch loss at step 4400: 17.068415\n",
      "Minibatch loss at step 4500: 16.510029\n",
      "Minibatch loss at step 4600: 15.566110\n",
      "Minibatch loss at step 4700: 14.711012\n",
      "Minibatch loss at step 4800: 14.054991\n",
      "Minibatch loss at step 4900: 13.410079\n",
      "Minibatch loss at step 5000: 12.847032\n",
      "Minibatch loss at step 5100: 12.228502\n",
      "Minibatch loss at step 5200: 11.667687\n",
      "Minibatch loss at step 5300: 11.014227\n",
      "Minibatch loss at step 5400: 10.555781\n",
      "Minibatch loss at step 5500: 10.085033\n",
      "Minibatch loss at step 5600: 9.479698\n",
      "Minibatch loss at step 5700: 9.033217\n",
      "Minibatch loss at step 5800: 8.521258\n",
      "Minibatch loss at step 5900: 8.343758\n",
      "Minibatch loss at step 6000: 7.990015\n",
      "Minibatch loss at step 6100: 7.538079\n",
      "Minibatch loss at step 6200: 7.097954\n",
      "Minibatch loss at step 6300: 6.871685\n",
      "Minibatch loss at step 6400: 6.511575\n",
      "Minibatch loss at step 6500: 6.136118\n",
      "Minibatch loss at step 6600: 5.943491\n",
      "Minibatch loss at step 6700: 5.554878\n",
      "Minibatch loss at step 6800: 5.558357\n",
      "Minibatch loss at step 6900: 5.179925\n",
      "Minibatch loss at step 7000: 4.993618\n",
      "Minibatch loss at step 7100: 4.627901\n",
      "Minibatch loss at step 7200: 4.658290\n",
      "Minibatch loss at step 7300: 4.311407\n",
      "Minibatch loss at step 7400: 4.081916\n",
      "Minibatch loss at step 7500: 3.957831\n",
      "Minibatch loss at step 7600: 3.708341\n",
      "Minibatch loss at step 7700: 3.665150\n",
      "Minibatch loss at step 7800: 3.407647\n",
      "Minibatch loss at step 7900: 3.246938\n",
      "Minibatch loss at step 8000: 3.322326\n",
      "Minibatch loss at step 8100: 2.904785\n",
      "Minibatch loss at step 8200: 2.831007\n",
      "Minibatch loss at step 8300: 2.725689\n",
      "Minibatch loss at step 8400: 2.556820\n",
      "Minibatch loss at step 8500: 2.512248\n",
      "Minibatch loss at step 8600: 2.390953\n",
      "Minibatch loss at step 8700: 2.325868\n",
      "Minibatch loss at step 8800: 2.210941\n",
      "Minibatch loss at step 8900: 2.112602\n",
      "Minibatch loss at step 9000: 2.184843\n",
      "Minibatch loss at step 9100: 1.978869\n",
      "Minibatch loss at step 9200: 1.922269\n",
      "Minibatch loss at step 9300: 1.801146\n",
      "Minibatch loss at step 9400: 1.709620\n",
      "Minibatch loss at step 9500: 1.729749\n",
      "Minibatch loss at step 9600: 1.640055\n",
      "Minibatch loss at step 9700: 1.520056\n",
      "Minibatch loss at step 9800: 1.513820\n",
      "Minibatch loss at step 9900: 1.576044\n",
      "Minibatch loss at step 10000: 1.501056\n",
      "Minibatch loss at step 10100: 1.417034\n",
      "Minibatch loss at step 10200: 1.445731\n",
      "Minibatch loss at step 10300: 1.417554\n",
      "Minibatch loss at step 10400: 1.163528\n",
      "Minibatch loss at step 10500: 1.211845\n",
      "Minibatch loss at step 10600: 1.187200\n",
      "Minibatch loss at step 10700: 1.257157\n",
      "Minibatch loss at step 10800: 1.107930\n",
      "Minibatch loss at step 10900: 1.029838\n",
      "Minibatch loss at step 11000: 1.001045\n",
      "Minibatch loss at step 11100: 0.986007\n",
      "Minibatch loss at step 11200: 1.016715\n",
      "Minibatch loss at step 11300: 0.822785\n",
      "Minibatch loss at step 11400: 0.964242\n",
      "Minibatch loss at step 11500: 0.992670\n",
      "Minibatch loss at step 11600: 0.785725\n",
      "Minibatch loss at step 11700: 0.808335\n",
      "Minibatch loss at step 11800: 0.826145\n",
      "Minibatch loss at step 11900: 0.821366\n",
      "Minibatch loss at step 12000: 0.934265\n",
      "Minibatch loss at step 12100: 0.699458\n",
      "Minibatch loss at step 12200: 0.661029\n",
      "Minibatch loss at step 12300: 0.743815\n",
      "Minibatch loss at step 12400: 0.690325\n",
      "Minibatch loss at step 12500: 0.679323\n",
      "Minibatch loss at step 12600: 0.782004\n",
      "Minibatch loss at step 12700: 0.696182\n",
      "Minibatch loss at step 12800: 0.579997\n",
      "Minibatch loss at step 12900: 0.731660\n",
      "Minibatch loss at step 13000: 0.829265\n",
      "Minibatch loss at step 13100: 0.753865\n",
      "Minibatch loss at step 13200: 0.660698\n",
      "Minibatch loss at step 13300: 0.770393\n",
      "Minibatch loss at step 13400: 0.505110\n",
      "Minibatch loss at step 13500: 0.599347\n",
      "Minibatch loss at step 13600: 0.665089\n",
      "Minibatch loss at step 13700: 0.685804\n",
      "Minibatch loss at step 13800: 0.513829\n",
      "Minibatch loss at step 13900: 0.564974\n",
      "Minibatch loss at step 14000: 0.636746\n",
      "Minibatch loss at step 14100: 0.598805\n",
      "Minibatch loss at step 14200: 0.599186\n",
      "Minibatch loss at step 14300: 0.459671\n",
      "Minibatch loss at step 14400: 0.620278\n",
      "Minibatch loss at step 14500: 0.644582\n",
      "Minibatch loss at step 14600: 0.492234\n",
      "Minibatch loss at step 14700: 0.488027\n",
      "Minibatch loss at step 14800: 0.502764\n",
      "Minibatch loss at step 14900: 0.560742\n",
      "Minibatch loss at step 15000: 0.538396\n",
      "Minibatch loss at step 15100: 0.669252\n",
      "Minibatch loss at step 15200: 0.594365\n",
      "Minibatch loss at step 15300: 0.554077\n",
      "Minibatch loss at step 15400: 0.474111\n",
      "Minibatch loss at step 15500: 0.528628\n",
      "Minibatch loss at step 15600: 0.449406\n",
      "Minibatch loss at step 15700: 0.456358\n",
      "Minibatch loss at step 15800: 0.462328\n",
      "Minibatch loss at step 15900: 0.458359\n",
      "Minibatch loss at step 16000: 0.444599\n",
      "Minibatch loss at step 16100: 0.378736\n",
      "Minibatch loss at step 16200: 0.434344\n",
      "Minibatch loss at step 16300: 0.542870\n",
      "Minibatch loss at step 16400: 0.465605\n",
      "Minibatch loss at step 16500: 0.455968\n",
      "Minibatch loss at step 16600: 0.567756\n",
      "Minibatch loss at step 16700: 0.515034\n",
      "Minibatch loss at step 16800: 0.595451\n",
      "Minibatch loss at step 16900: 0.362044\n",
      "Minibatch loss at step 17000: 0.438877\n",
      "Minibatch loss at step 17100: 0.371630\n",
      "Minibatch loss at step 17200: 0.429289\n",
      "Minibatch loss at step 17300: 0.554675\n",
      "Minibatch loss at step 17400: 0.456025\n",
      "Minibatch loss at step 17500: 0.296154\n",
      "Minibatch loss at step 17600: 0.499332\n",
      "Minibatch loss at step 17700: 0.398548\n",
      "Minibatch loss at step 17800: 0.495165\n",
      "Minibatch loss at step 17900: 0.501384\n",
      "Minibatch loss at step 18000: 0.386386\n",
      "Minibatch loss at step 18100: 0.363136\n",
      "Minibatch loss at step 18200: 0.534860\n",
      "Minibatch loss at step 18300: 0.395390\n",
      "Minibatch loss at step 18400: 0.462304\n",
      "Minibatch loss at step 18500: 0.481687\n",
      "Minibatch loss at step 18600: 0.545133\n",
      "Minibatch loss at step 18700: 0.564370\n",
      "Minibatch loss at step 18800: 0.487622\n",
      "Minibatch loss at step 18900: 0.432654\n",
      "Minibatch loss at step 19000: 0.350792\n",
      "Minibatch loss at step 19100: 0.428352\n",
      "Minibatch loss at step 19200: 0.652104\n",
      "Minibatch loss at step 19300: 0.572395\n",
      "Minibatch loss at step 19400: 0.384956\n",
      "Minibatch loss at step 19500: 0.428965\n",
      "Minibatch loss at step 19600: 0.488314\n",
      "Minibatch loss at step 19700: 0.543340\n",
      "Minibatch loss at step 19800: 0.324737\n",
      "Minibatch loss at step 19900: 0.553465\n",
      "Minibatch loss at step 20000: 0.559452\n",
      "Minibatch loss at step 20100: 0.482870\n",
      "Minibatch loss at step 20200: 0.572234\n",
      "Minibatch loss at step 20300: 0.465327\n",
      "Minibatch loss at step 20400: 0.438942\n",
      "Minibatch loss at step 20500: 0.479031\n",
      "Minibatch loss at step 20600: 0.464013\n",
      "Minibatch loss at step 20700: 0.408437\n",
      "Minibatch loss at step 20800: 0.488914\n",
      "Minibatch loss at step 20900: 0.535112\n",
      "Minibatch loss at step 21000: 0.391635\n",
      "Minibatch loss at step 21100: 0.456760\n",
      "Minibatch loss at step 21200: 0.517925\n",
      "Minibatch loss at step 21300: 0.670834\n",
      "Minibatch loss at step 21400: 0.406384\n",
      "Minibatch loss at step 21500: 0.363379\n",
      "Minibatch loss at step 21600: 0.468866\n",
      "Minibatch loss at step 21700: 0.318906\n",
      "Minibatch loss at step 21800: 0.500940\n",
      "Minibatch loss at step 21900: 0.413948\n",
      "Minibatch loss at step 22000: 0.281039\n",
      "Minibatch loss at step 22100: 0.573676\n",
      "Minibatch loss at step 22200: 0.430725\n",
      "Minibatch loss at step 22300: 0.411796\n",
      "Minibatch loss at step 22400: 0.415530\n",
      "Minibatch loss at step 22500: 0.403018\n",
      "Minibatch loss at step 22600: 0.542198\n",
      "Minibatch loss at step 22700: 0.424787\n",
      "Minibatch loss at step 22800: 0.439728\n",
      "Minibatch loss at step 22900: 0.466961\n",
      "Minibatch loss at step 23000: 0.407260\n",
      "Minibatch loss at step 23100: 0.424200\n",
      "Minibatch loss at step 23200: 0.413044\n",
      "Minibatch loss at step 23300: 0.475005\n",
      "Minibatch loss at step 23400: 0.455949\n",
      "Minibatch loss at step 23500: 0.333016\n",
      "Minibatch loss at step 23600: 0.529151\n",
      "Minibatch loss at step 23700: 0.483375\n",
      "Minibatch loss at step 23800: 0.433807\n",
      "Minibatch loss at step 23900: 0.580564\n",
      "Minibatch loss at step 24000: 0.352725\n",
      "Minibatch loss at step 24100: 0.470954\n",
      "Minibatch loss at step 24200: 0.482060\n",
      "Minibatch loss at step 24300: 0.476427\n",
      "Minibatch loss at step 24400: 0.327876\n",
      "Minibatch loss at step 24500: 0.473448\n",
      "Minibatch loss at step 24600: 0.427697\n",
      "Minibatch loss at step 24700: 0.367325\n",
      "Minibatch loss at step 24800: 0.408125\n",
      "Minibatch loss at step 24900: 0.411772\n",
      "Minibatch loss at step 25000: 0.383310\n",
      "Minibatch loss at step 25100: 0.466033\n",
      "Minibatch loss at step 25200: 0.335921\n",
      "Minibatch loss at step 25300: 0.379475\n",
      "Minibatch loss at step 25400: 0.546087\n",
      "Minibatch loss at step 25500: 0.427989\n",
      "Minibatch loss at step 25600: 0.405569\n",
      "Minibatch loss at step 25700: 0.410793\n",
      "Minibatch loss at step 25800: 0.355836\n",
      "Minibatch loss at step 25900: 0.327537\n",
      "Minibatch loss at step 26000: 0.318827\n",
      "Minibatch loss at step 26100: 0.349490\n",
      "Minibatch loss at step 26200: 0.521599\n",
      "Minibatch loss at step 26300: 0.492467\n",
      "Minibatch loss at step 26400: 0.342466\n",
      "Minibatch loss at step 26500: 0.439139\n",
      "Minibatch loss at step 26600: 0.484054\n",
      "Minibatch loss at step 26700: 0.398266\n",
      "Minibatch loss at step 26800: 0.432533\n",
      "Minibatch loss at step 26900: 0.440793\n",
      "Minibatch loss at step 27000: 0.429605\n",
      "Minibatch loss at step 27100: 0.512724\n",
      "Minibatch loss at step 27200: 0.365957\n",
      "Minibatch loss at step 27300: 0.539758\n",
      "Minibatch loss at step 27400: 0.490555\n",
      "Minibatch loss at step 27500: 0.514734\n",
      "Minibatch loss at step 27600: 0.435039\n",
      "Minibatch loss at step 27700: 0.444880\n",
      "Minibatch loss at step 27800: 0.431320\n",
      "Minibatch loss at step 27900: 0.413176\n",
      "Minibatch loss at step 28000: 0.421517\n",
      "Minibatch loss at step 28100: 0.426207\n",
      "Minibatch loss at step 28200: 0.341313\n",
      "Minibatch loss at step 28300: 0.496679\n",
      "Minibatch loss at step 28400: 0.350447\n",
      "Minibatch loss at step 28500: 0.344859\n",
      "Minibatch loss at step 28600: 0.439015\n",
      "Minibatch loss at step 28700: 0.483779\n",
      "Minibatch loss at step 28800: 0.377928\n",
      "Minibatch loss at step 28900: 0.328177\n",
      "Minibatch loss at step 29000: 0.435013\n",
      "Minibatch loss at step 29100: 0.374553\n",
      "Minibatch loss at step 29200: 0.359111\n",
      "Minibatch loss at step 29300: 0.547563\n",
      "Minibatch loss at step 29400: 0.429361\n",
      "Minibatch loss at step 29500: 0.474199\n",
      "Minibatch loss at step 29600: 0.390174\n",
      "Minibatch loss at step 29700: 0.382279\n",
      "Minibatch loss at step 29800: 0.400789\n",
      "Minibatch loss at step 29900: 0.422873\n",
      "Minibatch loss at step 30000: 0.319704\n",
      "Test accuracy: 94.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 30001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  lossVec = []\n",
    "  trainAcc = []\n",
    "  validAcc = []\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    lossVec.append(l)\n",
    "    if (step % 100 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      #print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      #print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "      \n",
    "      trainAcc.append(accuracy(predictions, batch_labels))\n",
    "      validAcc.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fName = 'Experiment_2.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(fName, 'wb')\n",
    "  save = {\n",
    "    'lossVec': lossVec,\n",
    "    'trainAcc': trainAcc,\n",
    "    'validAcc': validAcc,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'Experiment_2.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  lossVec = save['lossVec']\n",
    "  trainAcc = save['trainAcc']\n",
    "  validAcc = save['validAcc']\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEPCAYAAACHuClZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11395e510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEPCAYAAACHuClZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYRZMjDHGdJgFE2OMMR32/wCyM4xpiFpjTQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1442ad3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(lossVec)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,200])\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xrange(0,30001,100),trainAcc, label= \"Minibatch Accuracy\")\n",
    "plt.plot(xrange(0,30001,100),validAcc, label= \"Valid Accuracy\")\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([70,100])\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc=4)\n",
    "plt.savefig('Experiment_2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 18.152536\n",
      "Minibatch accuracy: 8.3%\n",
      "Validation accuracy: 10.8%\n",
      "Minibatch loss at step 500: 1.933836\n",
      "Minibatch accuracy: 91.7%\n",
      "Validation accuracy: 72.1%\n",
      "Minibatch loss at step 1000: 0.268556\n",
      "Minibatch accuracy: 91.7%\n",
      "Validation accuracy: 69.4%\n",
      "Minibatch loss at step 1500: 0.018284\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 2000: 4.813074\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.0%\n",
      "Minibatch loss at step 2500: 2.566367\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 3000: 2.651227\n",
      "Minibatch accuracy: 83.3%\n",
      "Validation accuracy: 75.3%\n",
      "Test accuracy: 83.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "hidden_layer_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weightsHidden = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_size]))\n",
    "  biasesHidden = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "  \n",
    "  logitsHidden = tf.matmul(tf_train_dataset, weightsHidden) + biasesHidden\n",
    "  hiddenLayer = tf.nn.relu(logitsHidden)\n",
    "\n",
    "  weights = tf.Variable(tf.truncated_normal([hidden_layer_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  # Training computation.\n",
    "  logits = tf.matmul(hiddenLayer, weights) + biases\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    " \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "  valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weightsHidden) + biasesHidden)\n",
    "  valid_logits = tf.matmul(valid_hidden, weights) + biases\n",
    "  valid_prediction = tf.nn.softmax(valid_logits)\n",
    "    \n",
    "  test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weightsHidden) + biasesHidden)\n",
    "  test_logits = tf.matmul(test_hidden, weights) + biases\n",
    "  test_prediction = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 343.291992\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 12.5%\n",
      "Minibatch loss at step 500: 403.188721\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 57.8%\n",
      "Minibatch loss at step 1000: 22.845827\n",
      "Minibatch accuracy: 58.3%\n",
      "Validation accuracy: 54.3%\n",
      "Minibatch loss at step 1500: 44.549931\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 56.5%\n",
      "Minibatch loss at step 2000: 3119.778076\n",
      "Minibatch accuracy: 66.7%\n",
      "Validation accuracy: 57.9%\n",
      "Minibatch loss at step 2500: 99.572533\n",
      "Minibatch accuracy: 41.7%\n",
      "Validation accuracy: 58.1%\n",
      "Minibatch loss at step 3000: 239.537170\n",
      "Minibatch accuracy: 66.7%\n",
      "Validation accuracy: 56.3%\n",
      "Test accuracy: 61.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weightsHidden = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_size]))\n",
    "  biasesHidden = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "  \n",
    "  logitsHidden = tf.matmul(tf_train_dataset, weightsHidden) + biasesHidden\n",
    "  hiddenLayer = tf.nn.relu(logitsHidden)\n",
    "\n",
    "  # Add a 50% dropout during training only. Dropout also scales\n",
    "  # activations such that no rescaling is needed at evaluation time.\n",
    "  hidden = tf.nn.dropout(hiddenLayer, 0.5, seed=None)\n",
    "\n",
    "  weights = tf.Variable(tf.truncated_normal([hidden_layer_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))  \n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(hidden, weights) + biases\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  regularizers = (tf.nn.l2_loss(weightsHidden) + tf.nn.l2_loss(biasesHidden) +\n",
    "                  tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases))\n",
    "  # Add the regularization term to the loss.\n",
    "  loss += 5e-4 * regularizers\n",
    " \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "  valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weightsHidden) + biasesHidden)\n",
    "  valid_logits = tf.matmul(valid_hidden, weights) + biases\n",
    "  valid_prediction = tf.nn.softmax(valid_logits)\n",
    "    \n",
    "  test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weightsHidden) + biasesHidden)\n",
    "  test_logits = tf.matmul(test_hidden, weights) + biases\n",
    "  test_prediction = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 621.514160\n",
      "Minibatch loss at step 100: 194.757156\n",
      "Minibatch loss at step 200: 229.357498\n",
      "Minibatch loss at step 300: 156.935684\n",
      "Minibatch loss at step 400: 163.726089\n",
      "Minibatch loss at step 500: 152.150177\n",
      "Minibatch loss at step 600: 137.132690\n",
      "Minibatch loss at step 700: 139.579590\n",
      "Minibatch loss at step 800: 113.138260\n",
      "Minibatch loss at step 900: 108.688881\n",
      "Minibatch loss at step 1000: 103.073929\n",
      "Minibatch loss at step 1100: 98.588264\n",
      "Minibatch loss at step 1200: 90.997398\n",
      "Minibatch loss at step 1300: 89.865631\n",
      "Minibatch loss at step 1400: 82.991379\n",
      "Minibatch loss at step 1500: 80.852081\n",
      "Minibatch loss at step 1600: 75.868774\n",
      "Minibatch loss at step 1700: 67.112907\n",
      "Minibatch loss at step 1800: 65.067574\n",
      "Minibatch loss at step 1900: 64.487663\n",
      "Minibatch loss at step 2000: 57.981853\n",
      "Minibatch loss at step 2100: 55.510502\n",
      "Minibatch loss at step 2200: 53.671085\n",
      "Minibatch loss at step 2300: 52.946709\n",
      "Minibatch loss at step 2400: 48.430599\n",
      "Minibatch loss at step 2500: 45.556419\n",
      "Minibatch loss at step 2600: 45.621735\n",
      "Minibatch loss at step 2700: 41.358440\n",
      "Minibatch loss at step 2800: 41.692612\n",
      "Minibatch loss at step 2900: 37.366436\n",
      "Minibatch loss at step 3000: 35.447159\n",
      "Minibatch loss at step 3100: 35.959270\n",
      "Minibatch loss at step 3200: 31.799719\n",
      "Minibatch loss at step 3300: 31.443823\n",
      "Minibatch loss at step 3400: 28.648863\n",
      "Minibatch loss at step 3500: 27.705681\n",
      "Minibatch loss at step 3600: 26.583782\n",
      "Minibatch loss at step 3700: 25.005049\n",
      "Minibatch loss at step 3800: 23.443773\n",
      "Minibatch loss at step 3900: 22.920591\n",
      "Minibatch loss at step 4000: 21.900145\n",
      "Minibatch loss at step 4100: 20.352606\n",
      "Minibatch loss at step 4200: 19.640656\n",
      "Minibatch loss at step 4300: 18.361238\n",
      "Minibatch loss at step 4400: 17.385529\n",
      "Minibatch loss at step 4500: 17.330091\n",
      "Minibatch loss at step 4600: 15.896831\n",
      "Minibatch loss at step 4700: 15.260895\n",
      "Minibatch loss at step 4800: 14.327396\n",
      "Minibatch loss at step 4900: 13.601097\n",
      "Minibatch loss at step 5000: 13.354417\n",
      "Minibatch loss at step 5100: 12.525083\n",
      "Minibatch loss at step 5200: 11.998086\n",
      "Minibatch loss at step 5300: 11.408918\n",
      "Minibatch loss at step 5400: 10.964459\n",
      "Minibatch loss at step 5500: 10.340522\n",
      "Minibatch loss at step 5600: 9.583470\n",
      "Minibatch loss at step 5700: 9.400201\n",
      "Minibatch loss at step 5800: 8.709751\n",
      "Minibatch loss at step 5900: 8.671045\n",
      "Minibatch loss at step 6000: 8.147923\n",
      "Minibatch loss at step 6100: 7.831837\n",
      "Minibatch loss at step 6200: 7.291117\n",
      "Minibatch loss at step 6300: 7.269668\n",
      "Minibatch loss at step 6400: 6.720672\n",
      "Minibatch loss at step 6500: 6.342323\n",
      "Minibatch loss at step 6600: 6.136275\n",
      "Minibatch loss at step 6700: 5.907058\n",
      "Minibatch loss at step 6800: 5.616608\n",
      "Minibatch loss at step 6900: 5.475343\n",
      "Minibatch loss at step 7000: 5.207915\n",
      "Minibatch loss at step 7100: 4.844721\n",
      "Minibatch loss at step 7200: 4.785740\n",
      "Minibatch loss at step 7300: 4.448714\n",
      "Minibatch loss at step 7400: 4.323411\n",
      "Minibatch loss at step 7500: 4.151240\n",
      "Minibatch loss at step 7600: 3.883758\n",
      "Minibatch loss at step 7700: 3.804448\n",
      "Minibatch loss at step 7800: 3.587902\n",
      "Minibatch loss at step 7900: 3.353679\n",
      "Minibatch loss at step 8000: 3.581122\n",
      "Minibatch loss at step 8100: 2.983373\n",
      "Minibatch loss at step 8200: 2.907664\n",
      "Minibatch loss at step 8300: 2.875605\n",
      "Minibatch loss at step 8400: 2.650175\n",
      "Minibatch loss at step 8500: 2.609362\n",
      "Minibatch loss at step 8600: 2.550410\n",
      "Minibatch loss at step 8700: 2.476077\n",
      "Minibatch loss at step 8800: 2.310898\n",
      "Minibatch loss at step 8900: 2.184510\n",
      "Minibatch loss at step 9000: 2.331249\n",
      "Minibatch loss at step 9100: 1.981747\n",
      "Minibatch loss at step 9200: 2.105913\n",
      "Minibatch loss at step 9300: 1.906928\n",
      "Minibatch loss at step 9400: 1.787919\n",
      "Minibatch loss at step 9500: 1.885061\n",
      "Minibatch loss at step 9600: 1.868528\n",
      "Minibatch loss at step 9700: 1.692905\n",
      "Minibatch loss at step 9800: 1.674640\n",
      "Minibatch loss at step 9900: 1.623847\n",
      "Minibatch loss at step 10000: 1.642802\n",
      "Minibatch loss at step 10100: 1.675421\n",
      "Minibatch loss at step 10200: 1.584092\n",
      "Minibatch loss at step 10300: 1.437966\n",
      "Minibatch loss at step 10400: 1.287362\n",
      "Minibatch loss at step 10500: 1.312662\n",
      "Minibatch loss at step 10600: 1.295107\n",
      "Minibatch loss at step 10700: 1.473182\n",
      "Minibatch loss at step 10800: 1.285383\n",
      "Minibatch loss at step 10900: 1.106327\n",
      "Minibatch loss at step 11000: 1.106197\n",
      "Minibatch loss at step 11100: 1.087547\n",
      "Minibatch loss at step 11200: 1.089547\n",
      "Minibatch loss at step 11300: 0.881194\n",
      "Minibatch loss at step 11400: 1.058152\n",
      "Minibatch loss at step 11500: 1.055490\n",
      "Minibatch loss at step 11600: 0.917073\n",
      "Minibatch loss at step 11700: 0.900080\n",
      "Minibatch loss at step 11800: 0.991079\n",
      "Minibatch loss at step 11900: 0.855416\n",
      "Minibatch loss at step 12000: 1.079020\n",
      "Minibatch loss at step 12100: 0.810208\n",
      "Minibatch loss at step 12200: 0.776283\n",
      "Minibatch loss at step 12300: 0.864164\n",
      "Minibatch loss at step 12400: 0.693073\n",
      "Minibatch loss at step 12500: 0.780543\n",
      "Minibatch loss at step 12600: 0.817565\n",
      "Minibatch loss at step 12700: 0.832260\n",
      "Minibatch loss at step 12800: 0.671998\n",
      "Minibatch loss at step 12900: 0.856701\n",
      "Minibatch loss at step 13000: 0.929692\n",
      "Minibatch loss at step 13100: 0.952496\n",
      "Minibatch loss at step 13200: 0.722531\n",
      "Minibatch loss at step 13300: 0.966214\n",
      "Minibatch loss at step 13400: 0.591469\n",
      "Minibatch loss at step 13500: 0.726192\n",
      "Minibatch loss at step 13600: 0.729382\n",
      "Minibatch loss at step 13700: 0.814040\n",
      "Minibatch loss at step 13800: 0.623561\n",
      "Minibatch loss at step 13900: 0.644316\n",
      "Minibatch loss at step 14000: 0.707872\n",
      "Minibatch loss at step 14100: 0.679210\n",
      "Minibatch loss at step 14200: 0.813469\n",
      "Minibatch loss at step 14300: 0.581089\n",
      "Minibatch loss at step 14400: 0.670111\n",
      "Minibatch loss at step 14500: 0.840967\n",
      "Minibatch loss at step 14600: 0.541363\n",
      "Minibatch loss at step 14700: 0.502287\n",
      "Minibatch loss at step 14800: 0.605199\n",
      "Minibatch loss at step 14900: 0.663561\n",
      "Minibatch loss at step 15000: 0.689611\n",
      "Minibatch loss at step 15100: 0.785795\n",
      "Minibatch loss at step 15200: 0.746925\n",
      "Minibatch loss at step 15300: 0.664561\n",
      "Minibatch loss at step 15400: 0.666278\n",
      "Minibatch loss at step 15500: 0.682144\n",
      "Minibatch loss at step 15600: 0.556593\n",
      "Minibatch loss at step 15700: 0.627157\n",
      "Minibatch loss at step 15800: 0.517763\n",
      "Minibatch loss at step 15900: 0.534784\n",
      "Minibatch loss at step 16000: 0.518875\n",
      "Minibatch loss at step 16100: 0.489675\n",
      "Minibatch loss at step 16200: 0.537332\n",
      "Minibatch loss at step 16300: 0.693297\n",
      "Minibatch loss at step 16400: 0.525035\n",
      "Minibatch loss at step 16500: 0.572598\n",
      "Minibatch loss at step 16600: 0.690949\n",
      "Minibatch loss at step 16700: 0.626758\n",
      "Minibatch loss at step 16800: 0.709893\n",
      "Minibatch loss at step 16900: 0.468202\n",
      "Minibatch loss at step 17000: 0.559101\n",
      "Minibatch loss at step 17100: 0.506564\n",
      "Minibatch loss at step 17200: 0.552950\n",
      "Minibatch loss at step 17300: 0.660186\n",
      "Minibatch loss at step 17400: 0.484190\n",
      "Minibatch loss at step 17500: 0.352982\n",
      "Minibatch loss at step 17600: 0.592774\n",
      "Minibatch loss at step 17700: 0.469895\n",
      "Minibatch loss at step 17800: 0.653544\n",
      "Minibatch loss at step 17900: 0.590248\n",
      "Minibatch loss at step 18000: 0.427712\n",
      "Minibatch loss at step 18100: 0.425438\n",
      "Minibatch loss at step 18200: 0.706335\n",
      "Minibatch loss at step 18300: 0.553397\n",
      "Minibatch loss at step 18400: 0.575839\n",
      "Minibatch loss at step 18500: 0.596539\n",
      "Minibatch loss at step 18600: 0.702666\n",
      "Minibatch loss at step 18700: 0.763657\n",
      "Minibatch loss at step 18800: 0.602487\n",
      "Minibatch loss at step 18900: 0.523509\n",
      "Minibatch loss at step 19000: 0.384703\n",
      "Minibatch loss at step 19100: 0.495624\n",
      "Minibatch loss at step 19200: 0.712804\n",
      "Minibatch loss at step 19300: 0.674477\n",
      "Minibatch loss at step 19400: 0.523109\n",
      "Minibatch loss at step 19500: 0.522663\n",
      "Minibatch loss at step 19600: 0.645335\n",
      "Minibatch loss at step 19700: 0.603591\n",
      "Minibatch loss at step 19800: 0.459937\n",
      "Minibatch loss at step 19900: 0.658055\n",
      "Minibatch loss at step 20000: 0.671342\n",
      "Minibatch loss at step 20100: 0.604396\n",
      "Minibatch loss at step 20200: 0.727416\n",
      "Minibatch loss at step 20300: 0.594203\n",
      "Minibatch loss at step 20400: 0.594069\n",
      "Minibatch loss at step 20500: 0.597727\n",
      "Minibatch loss at step 20600: 0.554674\n",
      "Minibatch loss at step 20700: 0.493765\n",
      "Minibatch loss at step 20800: 0.598661\n",
      "Minibatch loss at step 20900: 0.625916\n",
      "Minibatch loss at step 21000: 0.491510\n",
      "Minibatch loss at step 21100: 0.585623\n",
      "Minibatch loss at step 21200: 0.650836\n",
      "Minibatch loss at step 21300: 0.720500\n",
      "Minibatch loss at step 21400: 0.492746\n",
      "Minibatch loss at step 21500: 0.424420\n",
      "Minibatch loss at step 21600: 0.605975\n",
      "Minibatch loss at step 21700: 0.407053\n",
      "Minibatch loss at step 21800: 0.568116\n",
      "Minibatch loss at step 21900: 0.585831\n",
      "Minibatch loss at step 22000: 0.354063\n",
      "Minibatch loss at step 22100: 0.674320\n",
      "Minibatch loss at step 22200: 0.611257\n",
      "Minibatch loss at step 22300: 0.460910\n",
      "Minibatch loss at step 22400: 0.564162\n",
      "Minibatch loss at step 22500: 0.499848\n",
      "Minibatch loss at step 22600: 0.726408\n",
      "Minibatch loss at step 22700: 0.509640\n",
      "Minibatch loss at step 22800: 0.592747\n",
      "Minibatch loss at step 22900: 0.563520\n",
      "Minibatch loss at step 23000: 0.554793\n",
      "Minibatch loss at step 23100: 0.466474\n",
      "Minibatch loss at step 23200: 0.522199\n",
      "Minibatch loss at step 23300: 0.655667\n",
      "Minibatch loss at step 23400: 0.592156\n",
      "Minibatch loss at step 23500: 0.455337\n",
      "Minibatch loss at step 23600: 0.684438\n",
      "Minibatch loss at step 23700: 0.563401\n",
      "Minibatch loss at step 23800: 0.570681\n",
      "Minibatch loss at step 23900: 0.754103\n",
      "Minibatch loss at step 24000: 0.493593\n",
      "Minibatch loss at step 24100: 0.605881\n",
      "Minibatch loss at step 24200: 0.652495\n",
      "Minibatch loss at step 24300: 0.575073\n",
      "Minibatch loss at step 24400: 0.426391\n",
      "Minibatch loss at step 24500: 0.634270\n",
      "Minibatch loss at step 24600: 0.557264\n",
      "Minibatch loss at step 24700: 0.458355\n",
      "Minibatch loss at step 24800: 0.520938\n",
      "Minibatch loss at step 24900: 0.477809\n",
      "Minibatch loss at step 25000: 0.496078\n",
      "Minibatch loss at step 25100: 0.579396\n",
      "Minibatch loss at step 25200: 0.395289\n",
      "Minibatch loss at step 25300: 0.423249\n",
      "Minibatch loss at step 25400: 0.704016\n",
      "Minibatch loss at step 25500: 0.537561\n",
      "Minibatch loss at step 25600: 0.519312\n",
      "Minibatch loss at step 25700: 0.543636\n",
      "Minibatch loss at step 25800: 0.481421\n",
      "Minibatch loss at step 25900: 0.379475\n",
      "Minibatch loss at step 26000: 0.580858\n",
      "Minibatch loss at step 26100: 0.425010\n",
      "Minibatch loss at step 26200: 0.728788\n",
      "Minibatch loss at step 26300: 0.568529\n",
      "Minibatch loss at step 26400: 0.471171\n",
      "Minibatch loss at step 26500: 0.544991\n",
      "Minibatch loss at step 26600: 0.650087\n",
      "Minibatch loss at step 26700: 0.489672\n",
      "Minibatch loss at step 26800: 0.567092\n",
      "Minibatch loss at step 26900: 0.518894\n",
      "Minibatch loss at step 27000: 0.556363\n",
      "Minibatch loss at step 27100: 0.559390\n",
      "Minibatch loss at step 27200: 0.491229\n",
      "Minibatch loss at step 27300: 0.684647\n",
      "Minibatch loss at step 27400: 0.664996\n",
      "Minibatch loss at step 27500: 0.662969\n",
      "Minibatch loss at step 27600: 0.468012\n",
      "Minibatch loss at step 27700: 0.620111\n",
      "Minibatch loss at step 27800: 0.634477\n",
      "Minibatch loss at step 27900: 0.503402\n",
      "Minibatch loss at step 28000: 0.516741\n",
      "Minibatch loss at step 28100: 0.559057\n",
      "Minibatch loss at step 28200: 0.488054\n",
      "Minibatch loss at step 28300: 0.634267\n",
      "Minibatch loss at step 28400: 0.471096\n",
      "Minibatch loss at step 28500: 0.439878\n",
      "Minibatch loss at step 28600: 0.543740\n",
      "Minibatch loss at step 28700: 0.614231\n",
      "Minibatch loss at step 28800: 0.433691\n",
      "Minibatch loss at step 28900: 0.426532\n",
      "Minibatch loss at step 29000: 0.569477\n",
      "Minibatch loss at step 29100: 0.566556\n",
      "Minibatch loss at step 29200: 0.517223\n",
      "Minibatch loss at step 29300: 0.683132\n",
      "Minibatch loss at step 29400: 0.586895\n",
      "Minibatch loss at step 29500: 0.527478\n",
      "Minibatch loss at step 29600: 0.503467\n",
      "Minibatch loss at step 29700: 0.517575\n",
      "Minibatch loss at step 29800: 0.556310\n",
      "Minibatch loss at step 29900: 0.490594\n",
      "Minibatch loss at step 30000: 0.423666\n",
      "Test accuracy: 94.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 30001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  lossVec = []\n",
    "  trainAcc = []\n",
    "  validAcc = []\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    lossVec.append(l)\n",
    "    if (step % 100 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      #print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      #print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "      \n",
    "      trainAcc.append(accuracy(predictions, batch_labels))\n",
    "      validAcc.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fName = 'Experiment_3.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(fName, 'wb')\n",
    "  save = {\n",
    "    'lossVec': lossVec,\n",
    "    'trainAcc': trainAcc,\n",
    "    'validAcc': validAcc,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'Experiment_3.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  lossVec = save['lossVec']\n",
    "  trainAcc = save['trainAcc']\n",
    "  validAcc = save['validAcc']\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEPCAYAAACHuClZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGP5JREFUeJzt3X2QXXV9x/HPJ48kREJAQoAgT8GCT6VQkA44LiIRO8Wg\nVYqgYqgPI7XYsVaCpZPItNVQ2kqtzlihGlRIGVsFO5ZEwR15ChATDJgYQxGEmARKQjThcXe//eOc\nZe/dvXezm3vPPQ/3/Zq5s+f87jnnfn/3JPvd38M5xxEhAABaMSHvAAAA5UcyAQC0jGQCAGgZyQQA\n0DKSCQCgZSQTAEDLMk0mtufavt32z2w/aPvStHyW7ZW2N9peYXtmzT6X295ke4Pt+VnGBwBoD2d5\nnYntOZLmRMQDtmdI+omkBZIWSno6Iq6yfZmkWRGxyPZrJH1L0smS5kr6oaRjg4thAKDQMm2ZRMTW\niHggXd4laYOSJLFA0rJ0s2WSzk2X3yFpeUT0RcSjkjZJOiXLGAEArevYmIntIyWdIGmVpIMjYpuU\nJBxJs9PNDpP0eM1um9MyAECBdSSZpF1c35b0ibSFMrzbim4sACixSVl/gO1JShLJNyLi5rR4m+2D\nI2JbOq7yZFq+WdLhNbvPTcuGH5PkAwB7ISKcxXE70TL5d0nrI+KamrJbJH0wXb5I0s015efbnmL7\nKEnzJN3X+LBR9/r0p5OffX2hiHK/Fi9enHsM1I/6dWP9qly3iGz/Bs+0ZWL7NEkXSnrQ9lolv/k/\nI2mppJtsXyzpMUnnSVJErLd9k6T1kl6SdEmM8RvI+HsCAIwi02QSEXdJmtjk7bc22edzkj6XWVAA\ngLarzBXwVWqZ9PT05B1CpqhfuVW5flWuW9YyvWgxK8kAfH3cn/qUdPXVUl+fNLFZWwgAuphtRYkH\n4DuihDkRACqjMskEAJCfyiQTWiYAkJ/KJBMAQH4qk0xomQBAfiqTTAAA+alMMqFlAgD5qUwyAQDk\npzLJhJYJAOSnMskEAJCfyiQTWiYAkJ/KJBMAQH4qk0wGWya0UACg8yqTTAAA+alMMhlskTiTmysD\nAEZTmWQCAMhPZZLJihV5RwAA3asyT1ocxJMWAaAxnrQIACg0kgkAoGUkEwBAy0gmAICWkUwAAC0j\nmQAAWla5ZLJ1a94RAED3qVwyKeFlMwBQepVLJgCAzittMuGGjgBQHKVNJgCA4ihtMmnWMmHMBAA6\nr7TJ5IADGpffc09n4wAAlDiZXHFF43KmBgNA55U2mUyd2ri8v7+zcQAASpxMAADFQTIBALSstMmk\n2Wwurj8BgM4rbTJphqnBANB5lUsmAIDOI5kAAFpW2mQyobSRA0D1lPZX8vvfn3cEAIBBpU0m++yT\ndwQAgEGlTSYAgOKoXDJ58UVpYCDvKACguzhKeGGG7YiIphcoXnKJ9KUvdTYmACg624qITC7tzrRl\nYvs629tsr6spW2z7Cdtr0tfZNe9dbnuT7Q225+/t595xR6uRAwDGI+turq9JeluD8n+KiBPT162S\nZPt4SedJOl7S2yV92ebmKABQBpkmk4i4U9KOBm81ShILJC2PiL6IeFTSJkmnjHb8t7+95RABAG2Q\n1wD8x20/YPta2zPTssMkPV6zzea0rKklSxqX054BgM6alMNnflnSlRERtv9W0j9K+tB4D7JkyRJt\n3jy41pO+EuvWjdweALpNb2+vent7O/JZmc/msn2EpO9FxBtGe8/2IkkREUvT926VtDgi7m2wX0SE\n7r9fOqVJR1gJJ6kBQKZKO5srZdWMkdieU/PeuyQ9lC7fIul821NsHyVpnqT7RjswCQMAiiHTbi7b\nNyjpfzrQ9q8kLZZ0hu0TJA1IelTSRyUpItbbvknSekkvSbok9tBsIpkAQDGU+qLFe++VTj218TYl\nrBYAZKrs3VyZOfDAvCMAAEglTybz5kkPPbTn7QAA2Sp1MpGkyZPzjgAAUPpksu++o7/f18ddhAEg\na6VPJrNnNy7ftSv5efjh0oc/3Ll4AKAblT6ZTJ4svfnNI8u/853k59at0urVnY0JALpN6ZOJJC1Y\nkHcEANDdKpFMGnV1cbNHAOicSiSTCy4YWdbX1/k4AKBbVSKZNGqFfPKTnY8DALpVJZJJIzsaPZIL\nAJCJyiYTAEDnkEwAAC0jmQAAWkYyAQC0rNLJZOHCvCMAgO5Q6WTy9a/nHQEAdIdKJxMAQGeQTAAA\nLSOZAABaVplkcs45eUcAAN2rMsnk29/OOwIA6F6VSSZTpuQdAQB0r8okEwBAfkgmAICWdUUy4amL\nAJCtrkgmEXlHAADVVqlk8sUvNi5fv76zcQBAt3GU8M9229Eo7p07pf33b7xPCasJAG1lWxGRScd/\npZJJ8l7jfUpYTQBoqyyTSaW6uQAA+SCZAABaRjIBALSMZAIAaFnlkslRR+UdAQB0n8olkyuvzDsC\nAOg+lZsanLw/smxggNuqAOhuTA1ug6uuyjsCAKiurkkm69blHQEAVFclk8mFF44sK2FvHgCURiWT\nydKleUcAAN2lksnk0ENHlt14o3TNNZ2PBQC6QSWTSTNr1uQdAQBU05iSie1jbE9Nl3tsX2q7yc3e\ni2vjxrwjAIBqGtN1JrYfkPT7ko6U9H1JN0t6bUT8YabRNY9n1OtMkm0al//mN9IrXpFBUABQcEW4\nzmQgIvokvVPSFyPiryQdkkVAWRsYyDsCAKiesSaTl2y/V9JFkv47LZucTUjtsWNH3hEAQPcYazJZ\nKOkPJP1dRPzS9lGSvrGnnWxfZ3ub7XU1ZbNsr7S90fYK2zNr3rvc9ibbG2zPH29lajV7fC+3VAGA\n9hv3vblsz5J0eETs8Zpy26dL2iXp+oh4Q1q2VNLTEXGV7cskzYqIRbZfI+lbkk6WNFfSDyUd22hw\nZCxjJsl2I8vuuEM6/fQ97goAlZP7mIntXtv72T5A0hpJX7X9T3vaLyLulDS8w2mBpGXp8jJJ56bL\n75C0PCL6IuJRSZsknTKW+MbjTW9q9xEBAGPt5poZEb+R9C4lrYw3SnrrXn7m7IjYJkkRsVXS7LT8\nMEmP12y3OS3ba7NmtbI3AGCsJo11O9uHSDpP0l+3OYa9umvWkiVLXl7u6elRT0/PiG1+93el3t69\njAoASq63t1e9HfolONbrTN4j6W8k3RURH7N9tKR/iIg/HsO+R0j6Xs2YyQZJPRGxzfYcST+KiONt\nL5IUEbE03e5WSYsj4t4GxxzTmMnWrdIhDSYwc9NHAN0oyzGTzB+OZftIJcnk9en6UknbI2JpkwH4\nNyrp3vqBWhyAT7YdWRaRvPr7pUljbZsBQMkVYQB+ru3v2H4yff2n7blj2O8GSXdLerXtX9leKOnz\nks6yvVHSmem6ImK9pJskrVdylf0lY84Yo3jDGxqXL1smTS70lTIAUB5j/bv8a5JukPSedP19adlZ\no+0UERc0eavh4H1EfE7S58YY05gcckjjB2Nxny4AaJ+xzuY6KCK+lk7b7YuIr0s6KMO42qbBuDwA\noM3Gmkyetv0+2xPT1/skPZ1lYO3y0Y+OLGMAHgDaa6zJ5GIl04K3Stoi6d2SPphRTG01derIssEB\neABAe4wpmUTEYxHxjog4KCJmR8S5kvY4LbgIpk8fWfY//yOtWNH5WACgqvZ6arDtX0XEq9ocz1g/\ne1wTvUa7uSMtFADdIvepwU1w/10AgKTWkklp/qa/oNkEZQBAW4x6nYnt36px0rCkaZlElIEDD8w7\nAgCotsxvp5KF8Y6ZPPdc44F4iTETAN2jqGMmpTGtNG0oACinrkgmknTGGXlHAADV1RXdXJL02GPS\nkUeOLC9h9QFgr5T6FvRZ2Jtkkuw3sqyE1QeAvcKYCQCg0EgmAICWdVUyWblyZNnu3dLDD3c+FgCo\nkq4aM3nxxZF3ET7zTOm22xg7AVB9DMAPs7fJJNm3fn3GDGnXLpIJgOojmQzTzmQyqIRfAwCMC7O5\n2qjR8+ABAK3pupZJsv/IshJ+DQAwLrRMAACFRjIBALSsK5PJv/1b3hEAQLV05ZhJf780adhjwUr4\nNQDAuDBm0mYTJ44su/XWzscBAFXRlclEkhYurF9//PF84gCAKujaZPKlL9Wv9/fnEwcAVEHXJpPh\nj/J95pl84gCAKujKAfih49Svv/GN0qpVLR8WAAqJe3MNk1UykZjVBaC6mM2Vkc9+Nu8IAKAaurpl\n8tJL0pQp9WUDA83vLAwAZUbLJCOTJ48s27Sp83EAQNl1dTKRpLPOql//+c+l7dvziQUAyqqru7mG\njle/Pn++tGJF2w4PAIVAN1eHbd2adwQAUC60TCQdc4z0yCP1ZSX8WgBgVLRMMvbNb+YdAQCUGy2T\nl49Zv84UYQBVQ8ukAw49tH79Jz/JJw4AKCNaJqnf/lbab7/6shJ+NQDQFPfmGiaLZJIct369hF8N\nADRFNxcAoNBIJjXuuKN+/U//NJ84AKBs6OYacez69RJ+PQDQEN1cAIBCyy2Z2H7U9k9tr7V9X1o2\ny/ZK2xttr7A9s9NxPfZY/foTT3Q6AgAon9y6uWw/IumkiNhRU7ZU0tMRcZXtyyTNiohFDfbNrJsr\nOX79+s6dI6cNA0DZVLWbyw0+f4GkZenyMknndjSi1PDWycyZXA0PAKPJu2XyjKR+SV+JiGtt74iI\nWTXbbI+IAxrsm2nLJPmMkWUMxgMosyxbJpOyOOgYnRYRW2wfJGml7Y2Shv+6bvrre8mSJS8v9/T0\nqKenp63B/cu/SJde2tZDAkBH9fb2qre3tyOfVYipwbYXS9ol6UOSeiJim+05kn4UEcc32D7zlkny\nOfXrBfiqAGCvVW7MxPZ02zPS5X0lzZf0oKRbJH0w3ewiSTfnER8AYHzyGoA/WNKdttdKWiXpexGx\nUtJSSWelXV5nSvp8TvFJkq6+un79+efziQMAiq4Q3Vzj1alurghpQk26fetbpR/8IPOPBYBMcNfg\nYTqVTJLPql9fvVo66aSOfDQAtBXJZJhOJpP+fmnSsDlvJfzKAKB6A/BlMnFi3hEAQPGRTMZg1676\n9YsvzicOACgqurnG/Jn16y+8IE2Z0tEQAKAldHMVwD331K+fd14+cQBAEdEyGdfn1q+X8KsD0MVo\nmRTEli3169Om5RMHABQNyWQc5sypX3/+eR6eBQASyWTctm+vX1+4UOrryycWACgKxkz26vPr19/y\nFum22/KJBQDGiivgh8k7mTz1lDR79tD6vvuOvBYFAIqGZDJM3skkiaF+vYRfI4Auw2yuAhqePDZu\nzCcOACgCkkkLFi0aWj7uuPziAIC80c3Votrurp07pf32yy8WABgN3VwFtmPH0PLMmYydAOhOJJMW\n7b+/dNBBQ+uvfW1+sQBAXkgmbfDkk0PLGzZIt9+eXywAkAeSSZusWTO0fOaZ0rnn5hcLAHQaA/Bt\n9OEPS9deO7RewBABdDEuWhymqMlEqp/d9eCD0utel18sAFCLZDJMkZOJVJ9Qpk+Xdu/OLxYAGMTU\n4JJZt25o+dlnSSYAqo9kkoHXv176zGeG1mfMyC8WAOgEurkyxM0gARQJ3VwlNTx52NLAQD6xAECW\nSCYZG55QJk6UVq/OJxYAyArdXB1ClxeAvNHNVQF0eQGoMpJJBzXq8gKAKiCZdFhEciHjIFvavj2/\neACgHUgmOdi9W7r66qH1Aw+Unn46v3gAoFUkk5z85V9Kd9wxtP7KV0rLl+cXDwC0gtlcOYuQJkwY\nWQYA7cZsrgqzG8/0uuUWqb8/n5gAYLxomRTIk09KBx9cX/aLX0jHHptPPACqhZZJl5g9O2mlnHPO\nUNmrX801KQCKj5ZJQTUaSznoIOnXv5YmTconJgDlRsukCw2Opfzv/w6VPfWUNHly43EWAMgTyaTg\njj46SRxPPllfPmFCklRuvz2fuACgFt1cJfP889K0aY3fe+EFacqUzsYDoDzo5sLL9tknaalESBdf\nXP/e1KlJa8VmWjGAziKZlNh11yVJ5aWXRrZWJk0aSiw33JBPfAC6B8mkAiZNkp59NkksAwPSu99d\n//6FFw4lFls64ABp5858YgVQTYyZVNzDD4/tosff+R3p+9+XjjiCW+MDVdV1Yya2z7b9c9u/sH1Z\n3vGU2bx5Q2MsEUkL5vjjR263caN0zDH13WODr4sukp55RnrxRaYkA2iscMnE9gRJ/yrpbZJeK+m9\nto/LN6rO6u3tzezY06ZJ69fXJ5j+fukrX2m+z/XXS7NmJQP8g1OSG70WLZLuukvavDkZx2kmy/oV\nAfUrryrXLWuFSyaSTpG0KSIei4iXJC2XtCDnmDqq0/+gJ0yQPvKR+gQTIfX1SVu3Sn/xF2M7ztKl\n0umnS3PnJlOUmyWdM87obfqeLV1wgfTOd0pXXCF94QvS4sXS6tXSE09IW7YMxbd7dxLjwMBQi6mv\nbyiewe06req/kKpcvyrXLWtFvDHHYZIer1l/QkmCQYdNnJjcePKf/zl5NROR3JDy7rtHTlfeGzfe\nmPz87neHyq68svXjdtJnP5t3BNmqcv2qXLcsFbFlgpKxkwH8hQtHtm4avRYvbv7ec89Ja9Ykt+D/\n6leTls6rXpV3DQHsSeFmc9k+VdKSiDg7XV8kKSJiac02xQoaAEoiq9lcRUwmEyVtlHSmpC2S7pP0\n3ojYkGtgAICmCjdmEhH9tj8uaaWSbrjrSCQAUGyFa5kAAMqndAPwZb2g0fajtn9qe63t+9KyWbZX\n2t5oe4XtmTXbX257k+0NtufXlJ9oe11a/y/kUZc0jutsb7O9rqasbfWxPcX28nSfe2x3dBi+Sf0W\n237C9pr0dXbNe6Wpn+25tm+3/TPbD9q+NC2vxPlrUL8/T8urcv6m2r43/V3yM9t/n5bne/4iojQv\nJcnvYUlHSJos6QFJx+Ud1xhjf0TSrGFlSyV9Ol2+TNLn0+XXSFqrpBvyyLTOg63IeyWdnC5/X9Lb\ncqrP6ZJOkLQui/pI+pikL6fLfyJpeQHqt1jSJxtse3yZ6idpjqQT0uUZSsYoj6vK+RulfpU4f+ln\nTk9/TpS0StJpeZ+/srVMynxBozWyJbhA0rJ0eZmkc9Pldyg5eX0R8aikTZJOsT1H0isi4v50u+tr\n9umoiLhT0o5hxe2sT+2xvq1kQkbHNKmflJzH4RaoRPWLiK0R8UC6vEvSBklzVZHz16R+h6Vvl/78\nSVJEPJsuTlXye2WHcj5/ZUsmjS5oPKzJtkUTkn5g+37bH0rLDo6IbVLyH0DS7LR8eD03p2WHKanz\noKLVf3Yb6/PyPhHRL+kZ2wdkF/qYfdz2A7avrelGKG39bB+ppAW2Su3991i0+t2bFlXi/NmeYHut\npK2SeiNivXI+f2VLJmV2WkScKOkPJf2Z7TcpSTC1qjYbop31yWRu/Dh9WdLREXGCkv/E/9jGY3e8\nfrZnKPmr8xPpX/BZ/nssQv0qc/4iYiAifk9Ji/JNtnuU8/krWzLZLKl2IGhuWlZ4EbEl/fmUpO8q\n6bLbZvtgSUqbnINPet8s6fCa3Qfr2ay8KNpZn5ffc3Lt0X4RsT270PcsIp6KtBNZ0lc1dJuf0tXP\n9iQlv2i/ERE3p8WVOX+N6lel8zcoIn6jZKzj95Xz+StbMrlf0jzbR9ieIul8SbfkHNMe2Z6e/pUk\n2/tKmi/pQSWxfzDd7CJJg/+pb5F0fjqj4ihJ8yTdlzZdd9o+xbYlfaBmnzxY9X+xtLM+t6THkKT3\nSLo9s1o0V1e/9D/ooHdJeihdLmP9/l3S+oi4pqasSudvRP2qcv5sv3Kwi872NElnKRlgz/f8dXIG\nQjteks5WMjtjk6RFecczxpiPUjLzbK2SJLIoLT9A0g/T+qyUtH/NPpcrmXWxQdL8mvKT0mNsknRN\njnW6QdKvJb0g6VeSFkqa1a76KBlYvCktXyXpyALU73pJ69Jz+V0lfdSlq5+SmT/9Nf8m16T/r9r2\n77Gg9avK+Xt9Wqe1kn4q6VNpea7nj4sWAQAtK1s3FwCggEgmAICWkUwAAC0jmQAAWkYyAQC0jGQC\nAGgZyQQYhe2/tv2Qk8cHrLF9su1P2N4n79iAIuE6E6AJ26cquX/TmyOiL73R3VRJd0s6KXK+vQtQ\nJLRMgOYOkfR/EdEnSWnyeLekQyX9yPZtkmR7vu27ba+2/R+2p6flv7S9NH340CrbR6fl73Hy0Ka1\ntntzqRnQZrRMgCbS+6jdKWmapNsk/UdE/Nj2I0paJjtsHyjpvySdHRHP2f60pCkR8be2fynpKxHx\nedvvl3ReRJzj5OmNb4uILbb3i+RmfUCp0TIBmoiI3ZJOlPQRSU9JWm578OZ3gzeAPFXJk+zuSp8v\n8QHV39l6efrzxnRbSbpL0rL0uTaTsqsB0Dn8QwZGEUnT/ceSfmz7QQ3dSXWQJa2MiAubHWL4ckR8\nzPbJkv5I0k9snxgRjZ7qCJQGLROgCduvtj2vpugESY9K+q2k/dKyVZJOs31Mus9028fW7PMn6c/z\nJd2TbnN0RNwfEYuVPHOi9pkSQCnRMgGamyHpi+mzI/qU3ML7I5IukHSr7c0RcabthZJutD1VSevj\nCiW37pakWbZ/Kul5Se9Ny/6hJuH8MCLWdag+QGYYgAcykg7AM4UYXYFuLiA7/KWGrkHLBADQMlom\nAICWkUwAAC0jmQAAWkYyAQC0jGQCAGgZyQQA0LL/B37zBW1QWbzyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14444b550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEPCAYAAACHuClZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1443b7050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(lossVec)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,200])\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xrange(0,30001,100),trainAcc, label= \"Minibatch Accuracy\")\n",
    "plt.plot(xrange(0,30001,100),validAcc, label= \"Valid Accuracy\")\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([70,100])\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc=4)\n",
    "plt.savefig('Experiment_3.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use learning rate decay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weightsHidden = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_size]))\n",
    "  biasesHidden = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "  \n",
    "  logitsHidden = tf.matmul(tf_train_dataset, weightsHidden) + biasesHidden\n",
    "  hiddenLayer = tf.nn.relu(logitsHidden)\n",
    "\n",
    "\n",
    "\n",
    "  # Add a 50% dropout during training only. Dropout also scales\n",
    "  # activations such that no rescaling is needed at evaluation time.\n",
    "  hidden = tf.nn.dropout(hiddenLayer, 0.5)\n",
    "\n",
    "  weights = tf.Variable(tf.truncated_normal([hidden_layer_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))  \n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(hidden, weights) + biases\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  regularizers = (tf.nn.l2_loss(weightsHidden) + tf.nn.l2_loss(biasesHidden) +\n",
    "                  tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases))\n",
    "  # Add the regularization term to the loss.\n",
    "  loss += 5e-4 * regularizers\n",
    " \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1, 0.9999)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "  valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weightsHidden) + biasesHidden)\n",
    "  valid_logits = tf.matmul(valid_hidden, weights) + biases\n",
    "  valid_prediction = tf.nn.softmax(valid_logits)\n",
    "    \n",
    "  test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weightsHidden) + biasesHidden)\n",
    "  test_logits = tf.matmul(test_hidden, weights) + biases\n",
    "  test_prediction = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 586.644470\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 31.1%\n",
      "Minibatch loss at step 500: 144.450104\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 1000: 108.369019\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1500: 81.405472\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 2000: 64.280525\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2500: 52.148396\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 3000: 42.689499\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 3500: 36.635384\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 4000: 30.571907\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 4500: 25.554876\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 5000: 22.136137\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 5500: 18.841969\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 6000: 16.455284\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 6500: 14.419583\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 7000: 12.825460\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 7500: 11.518260\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 8000: 10.477324\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 8500: 9.053788\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 9000: 8.460045\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 9500: 7.516630\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 10000: 6.930431\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.0%\n",
      "Test accuracy: 93.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper Network along with learning rate decay:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First try: 2 relu hidden layers with 1024 nodes. Use dropout 50%. Use learning rate decay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math as math\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Hidden Layer 1, 1024 nodes\n",
    "  weightsHidden1 = tf.Variable(tf.truncated_normal([image_size * image_size, 1024], stddev=math.sqrt(2.0/1024)))\n",
    "  biasesHidden1 = tf.Variable(tf.zeros([1024]))\n",
    "  logitsHidden1 = tf.matmul(tf_train_dataset, weightsHidden1) + biasesHidden1\n",
    "  hiddenLayer1 = tf.nn.relu(logitsHidden1)\n",
    "\n",
    "  # Hidden Layer 2, 1024 nodes\n",
    "  weightsHidden2 = tf.Variable(tf.truncated_normal([1024, 1024], stddev=math.sqrt(2.0/1024)))\n",
    "  biasesHidden2 = tf.Variable(tf.zeros([1024]))\n",
    "  logitsHidden2 = tf.matmul(hiddenLayer1, weightsHidden2) + biasesHidden2\n",
    "  hiddenLayer2 = tf.nn.relu(logitsHidden2)\n",
    "\n",
    "\n",
    "  # Add a 50% dropout during training only. Dropout also scales\n",
    "  # activations such that no rescaling is needed at evaluation time.\n",
    "  hidden = tf.nn.dropout(hiddenLayer2, 0.5)\n",
    "\n",
    "  # Training computation.\n",
    "  weights = tf.Variable(tf.truncated_normal([1024, num_labels], stddev=math.sqrt(2.0/1024)))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))  \n",
    "  logits = tf.matmul(hidden, weights) + biases\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  regularizers = (tf.nn.l2_loss(weightsHidden1) + tf.nn.l2_loss(biasesHidden1) +\n",
    "                  tf.nn.l2_loss(weightsHidden2) + tf.nn.l2_loss(biasesHidden2) +\n",
    "                  tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases))\n",
    "  # Add the regularization term to the loss.\n",
    "  loss += 5e-4 * regularizers\n",
    " \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1, 0.9999)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "  valid_hidden1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weightsHidden1) + biasesHidden1)\n",
    "  valid_hidden2 = tf.nn.relu(tf.matmul(valid_hidden1, weightsHidden2) + biasesHidden2)\n",
    "  valid_logits = tf.matmul(valid_hidden2, weights) + biases\n",
    "  valid_prediction = tf.nn.softmax(valid_logits)\n",
    "    \n",
    "  test_hidden1 = tf.nn.relu(tf.matmul(tf_test_dataset, weightsHidden1) + biasesHidden1)\n",
    "  test_hidden2 = tf.nn.relu(tf.matmul(test_hidden1, weightsHidden2) + biasesHidden2)\n",
    "  test_logits = tf.matmul(test_hidden2, weights) + biases\n",
    "  test_prediction = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.139753\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 34.0%\n",
      "Minibatch loss at step 500: 0.939973\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 1000: 0.942260\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 1500: 0.681094\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 2000: 0.626840\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 2500: 0.630180\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 3000: 0.622546\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 3500: 0.644756\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 4000: 0.517130\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 4500: 0.456688\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 5000: 0.518913\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 5500: 0.514398\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 6000: 0.576507\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 6500: 0.419788\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 7000: 0.515921\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 7500: 0.510859\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 8000: 0.573004\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 8500: 0.379195\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 9000: 0.457010\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 9500: 0.434921\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 10000: 0.389254\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.2%\n",
      "Final Test accuracy: 95.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Final Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as math\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Hidden Layer 1, 1024 nodes\n",
    "  h1_size = 1024\n",
    "  weightsHidden1 = tf.Variable(tf.truncated_normal([image_size * image_size, h1_size], stddev=math.sqrt(2.0/(image_size * image_size))))\n",
    "  biasesHidden1 = tf.Variable(tf.zeros([h1_size]))\n",
    "  logitsHidden1 = tf.matmul(tf_train_dataset, weightsHidden1) + biasesHidden1\n",
    "  hiddenLayer1 = tf.nn.relu(logitsHidden1)\n",
    "\n",
    "  # Hidden Layer 2, 1024 nodes\n",
    "  h2_size = 1024\n",
    "  weightsHidden2 = tf.Variable(tf.truncated_normal([h1_size, h2_size], stddev=math.sqrt(2.0/h1_size)))\n",
    "  biasesHidden2 = tf.Variable(tf.zeros([h2_size]))\n",
    "  logitsHidden2 = tf.matmul(hiddenLayer1, weightsHidden2) + biasesHidden2\n",
    "  hiddenLayer2 = tf.nn.relu(logitsHidden2)\n",
    "\n",
    "  # Hidden Layer 3, 305 nodes\n",
    "  h3_size = 305\n",
    "  weightsHidden3 = tf.Variable(tf.truncated_normal([h2_size, h3_size], stddev=math.sqrt(2.0/h2_size)))\n",
    "  biasesHidden3 = tf.Variable(tf.zeros([h3_size]))\n",
    "  logitsHidden3 = tf.matmul(hiddenLayer2, weightsHidden3) + biasesHidden3\n",
    "  hiddenLayer3 = tf.nn.relu(logitsHidden3)\n",
    "    \n",
    "  # Hidden Layer 4, 75 nodes\n",
    "  h4_size = 75\n",
    "  weightsHidden4 = tf.Variable(tf.truncated_normal([h3_size, h4_size], stddev=math.sqrt(2.0/h3_size)))\n",
    "  biasesHidden4 = tf.Variable(tf.zeros([h4_size]))\n",
    "  logitsHidden4 = tf.matmul(hiddenLayer3, weightsHidden4) + biasesHidden4\n",
    "  hiddenLayer4 = tf.nn.relu(logitsHidden4)\n",
    "\n",
    "\n",
    "  # Add a 50% dropout during training only. Dropout also scales\n",
    "  # activations such that no rescaling is needed at evaluation time.\n",
    "  hidden = tf.nn.dropout(hiddenLayer4, 0.5)\n",
    "\n",
    "  # Training computation.\n",
    "  weights = tf.Variable(tf.truncated_normal([h4_size, num_labels], stddev=math.sqrt(2.0/h4_size)))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))  \n",
    "  logits = tf.matmul(hidden, weights) + biases\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  regularizers = (tf.nn.l2_loss(weightsHidden1) + tf.nn.l2_loss(biasesHidden1) +\n",
    "                  tf.nn.l2_loss(weightsHidden2) + tf.nn.l2_loss(biasesHidden2) +\n",
    "                  tf.nn.l2_loss(weightsHidden3) + tf.nn.l2_loss(biasesHidden3) +\n",
    "                  tf.nn.l2_loss(weightsHidden4) + tf.nn.l2_loss(biasesHidden4) +\n",
    "                  tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases))\n",
    "  # Add the regularization term to the loss.\n",
    "  loss += 1e-5 * regularizers\n",
    " \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1, 0.9999)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "  valid_hidden1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weightsHidden1) + biasesHidden1)\n",
    "  valid_hidden2 = tf.nn.relu(tf.matmul(valid_hidden1, weightsHidden2) + biasesHidden2)\n",
    "  valid_hidden3 = tf.nn.relu(tf.matmul(valid_hidden2, weightsHidden3) + biasesHidden3)\n",
    "  valid_hidden4 = tf.nn.relu(tf.matmul(valid_hidden3, weightsHidden4) + biasesHidden4)\n",
    "  valid_logits = tf.matmul(valid_hidden4, weights) + biases\n",
    "  valid_prediction = tf.nn.softmax(valid_logits)\n",
    "    \n",
    "  test_hidden1 = tf.nn.relu(tf.matmul(tf_test_dataset, weightsHidden1) + biasesHidden1)\n",
    "  test_hidden2 = tf.nn.relu(tf.matmul(test_hidden1, weightsHidden2) + biasesHidden2)\n",
    "  test_hidden3 = tf.nn.relu(tf.matmul(test_hidden2, weightsHidden3) + biasesHidden3)\n",
    "  test_hidden4 = tf.nn.relu(tf.matmul(test_hidden3, weightsHidden4) + biasesHidden4)\n",
    "  test_logits = tf.matmul(test_hidden4, weights) + biases\n",
    "  test_prediction = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.353669\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 18.8%\n",
      "Minibatch loss at step 500: 0.418373\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 1000: 0.519943\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 1500: 0.363477\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 2000: 0.284200\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 2500: 0.345797\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 3000: 0.385284\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 3500: 0.403226\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 4000: 0.322997\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 4500: 0.278545\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 5000: 0.316277\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 5500: 0.318955\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 6000: 0.450437\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 6500: 0.254031\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 7000: 0.374473\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 7500: 0.298691\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 8000: 0.418372\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 8500: 0.161370\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 9000: 0.221178\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 9500: 0.237966\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 10000: 0.233007\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.6%\n",
      "Final Test accuracy: 96.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Final Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as math\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Hidden Layer 1, 2048 nodes\n",
    "  h1_size = 2048\n",
    "  weightsHidden1 = tf.Variable(tf.truncated_normal([image_size * image_size, h1_size], stddev=math.sqrt(2.0/(image_size * image_size))))\n",
    "  biasesHidden1 = tf.Variable(tf.zeros([h1_size]))\n",
    "  logitsHidden1 = tf.matmul(tf_train_dataset, weightsHidden1) + biasesHidden1\n",
    "  hiddenLayer1 = tf.nn.relu(logitsHidden1)\n",
    "\n",
    "  # Hidden Layer 2, 1024 nodes\n",
    "  h2_size = 1024\n",
    "  weightsHidden2 = tf.Variable(tf.truncated_normal([h1_size, h2_size], stddev=math.sqrt(2.0/h1_size)))\n",
    "  biasesHidden2 = tf.Variable(tf.zeros([h2_size]))\n",
    "  logitsHidden2 = tf.matmul(hiddenLayer1, weightsHidden2) + biasesHidden2\n",
    "  hiddenLayer2 = tf.nn.relu(logitsHidden2)\n",
    "\n",
    "  # Hidden Layer 3, 1024 nodes\n",
    "  h3_size = 1024\n",
    "  weightsHidden3 = tf.Variable(tf.truncated_normal([h2_size, h3_size], stddev=math.sqrt(2.0/h2_size)))\n",
    "  biasesHidden3 = tf.Variable(tf.zeros([h3_size]))\n",
    "  logitsHidden3 = tf.matmul(hiddenLayer2, weightsHidden3) + biasesHidden3\n",
    "  hiddenLayer3 = tf.nn.relu(logitsHidden3)\n",
    "    \n",
    "  # Hidden Layer 4, 1024 nodes\n",
    "  h4_size = 1024\n",
    "  weightsHidden4 = tf.Variable(tf.truncated_normal([h3_size, h4_size], stddev=math.sqrt(2.0/h3_size)))\n",
    "  biasesHidden4 = tf.Variable(tf.zeros([h4_size]))\n",
    "  logitsHidden4 = tf.matmul(hiddenLayer3, weightsHidden4) + biasesHidden4\n",
    "  hiddenLayer4 = tf.nn.relu(logitsHidden4)\n",
    "\n",
    "  # Add a 50% dropout during training only. Dropout also scales\n",
    "  # activations such that no rescaling is needed at evaluation time.\n",
    "  hidden = tf.nn.dropout(hiddenLayer4, 0.5)\n",
    "\n",
    "  # Training computation.\n",
    "  weights = tf.Variable(tf.truncated_normal([h4_size, num_labels], stddev=math.sqrt(2.0/h4_size)))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))  \n",
    "  logits = tf.matmul(hidden, weights) + biases\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  regularizers = (tf.nn.l2_loss(weightsHidden1) + tf.nn.l2_loss(biasesHidden1) +\n",
    "                  tf.nn.l2_loss(weightsHidden2) + tf.nn.l2_loss(biasesHidden2) +\n",
    "                  tf.nn.l2_loss(weightsHidden3) + tf.nn.l2_loss(biasesHidden3) +\n",
    "                  tf.nn.l2_loss(weightsHidden4) + tf.nn.l2_loss(biasesHidden4) +\n",
    "                  tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases))\n",
    "  # Add the regularization term to the loss.\n",
    "  loss += 1e-5 * regularizers\n",
    " \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.95)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "  valid_hidden1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weightsHidden1) + biasesHidden1)\n",
    "  valid_hidden2 = tf.nn.relu(tf.matmul(valid_hidden1, weightsHidden2) + biasesHidden2)\n",
    "  valid_hidden3 = tf.nn.relu(tf.matmul(valid_hidden2, weightsHidden3) + biasesHidden3)\n",
    "  valid_hidden4 = tf.nn.relu(tf.matmul(valid_hidden3, weightsHidden4) + biasesHidden4)\n",
    "  valid_logits = tf.matmul(valid_hidden4, weights) + biases\n",
    "  valid_prediction = tf.nn.softmax(valid_logits)\n",
    "    \n",
    "  test_hidden1 = tf.nn.relu(tf.matmul(tf_test_dataset, weightsHidden1) + biasesHidden1)\n",
    "  test_hidden2 = tf.nn.relu(tf.matmul(test_hidden1, weightsHidden2) + biasesHidden2)\n",
    "  test_hidden3 = tf.nn.relu(tf.matmul(test_hidden2, weightsHidden3) + biasesHidden3)\n",
    "  test_hidden4 = tf.nn.relu(tf.matmul(test_hidden3, weightsHidden4) + biasesHidden4)\n",
    "  test_logits = tf.matmul(test_hidden4, weights) + biases\n",
    "  test_prediction = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.436976\n",
      "Minibatch learning rate at step 0: 0.500000\n",
      "Minibatch loss at step 200: 0.587807\n",
      "Minibatch learning rate at step 200: 0.494897\n",
      "Minibatch loss at step 400: 0.510476\n",
      "Minibatch learning rate at step 400: 0.489846\n",
      "Minibatch loss at step 600: 0.567313\n",
      "Minibatch learning rate at step 600: 0.484846\n",
      "Minibatch loss at step 800: 0.383878\n",
      "Minibatch learning rate at step 800: 0.479898\n",
      "Minibatch loss at step 1000: 0.513349\n",
      "Minibatch learning rate at step 1000: 0.475000\n",
      "Minibatch loss at step 1200: 0.369692\n",
      "Minibatch learning rate at step 1200: 0.470152\n",
      "Minibatch loss at step 1400: 0.368005\n",
      "Minibatch learning rate at step 1400: 0.465354\n",
      "Minibatch loss at step 1600: 0.437011\n",
      "Minibatch learning rate at step 1600: 0.460604\n",
      "Minibatch loss at step 1800: 0.402134\n",
      "Minibatch learning rate at step 1800: 0.455903\n",
      "Minibatch loss at step 2000: 0.260245\n",
      "Minibatch learning rate at step 2000: 0.451250\n",
      "Minibatch loss at step 2200: 0.391105\n",
      "Minibatch learning rate at step 2200: 0.446644\n",
      "Minibatch loss at step 2400: 0.346583\n",
      "Minibatch learning rate at step 2400: 0.442086\n",
      "Minibatch loss at step 2600: 0.406325\n",
      "Minibatch learning rate at step 2600: 0.437574\n",
      "Minibatch loss at step 2800: 0.364020\n",
      "Minibatch learning rate at step 2800: 0.433108\n",
      "Minibatch loss at step 3000: 0.361565\n",
      "Minibatch learning rate at step 3000: 0.428687\n",
      "Minibatch loss at step 3200: 0.254139\n",
      "Minibatch learning rate at step 3200: 0.424312\n",
      "Minibatch loss at step 3400: 0.322028\n",
      "Minibatch learning rate at step 3400: 0.419982\n",
      "Minibatch loss at step 3600: 0.388070\n",
      "Minibatch learning rate at step 3600: 0.415695\n",
      "Minibatch loss at step 3800: 0.208981\n",
      "Minibatch learning rate at step 3800: 0.411452\n",
      "Minibatch loss at step 4000: 0.300110\n",
      "Minibatch learning rate at step 4000: 0.407253\n",
      "Minibatch loss at step 4200: 0.248248\n",
      "Minibatch learning rate at step 4200: 0.403097\n",
      "Minibatch loss at step 4400: 0.241519\n",
      "Minibatch learning rate at step 4400: 0.398982\n",
      "Minibatch loss at step 4600: 0.318495\n",
      "Minibatch learning rate at step 4600: 0.394910\n",
      "Minibatch loss at step 4800: 0.308474\n",
      "Minibatch learning rate at step 4800: 0.390880\n",
      "Minibatch loss at step 5000: 0.324617\n",
      "Minibatch learning rate at step 5000: 0.386890\n",
      "Minibatch loss at step 5200: 0.313044\n",
      "Minibatch learning rate at step 5200: 0.382942\n",
      "Minibatch loss at step 5400: 0.197774\n",
      "Minibatch learning rate at step 5400: 0.379033\n",
      "Minibatch loss at step 5600: 0.312163\n",
      "Minibatch learning rate at step 5600: 0.375165\n",
      "Minibatch loss at step 5800: 0.210676\n",
      "Minibatch learning rate at step 5800: 0.371336\n",
      "Minibatch loss at step 6000: 0.396383\n",
      "Minibatch learning rate at step 6000: 0.367546\n",
      "Minibatch loss at step 6200: 0.166834\n",
      "Minibatch learning rate at step 6200: 0.363795\n",
      "Minibatch loss at step 6400: 0.237362\n",
      "Minibatch learning rate at step 6400: 0.360082\n",
      "Minibatch loss at step 6600: 0.220753\n",
      "Minibatch learning rate at step 6600: 0.356407\n",
      "Minibatch loss at step 6800: 0.267924\n",
      "Minibatch learning rate at step 6800: 0.352769\n",
      "Minibatch loss at step 7000: 0.320786\n",
      "Minibatch learning rate at step 7000: 0.349169\n",
      "Minibatch loss at step 7200: 0.296941\n",
      "Minibatch learning rate at step 7200: 0.345605\n",
      "Minibatch loss at step 7400: 0.159148\n",
      "Minibatch learning rate at step 7400: 0.342078\n",
      "Minibatch loss at step 7600: 0.196835\n",
      "Minibatch learning rate at step 7600: 0.338586\n",
      "Minibatch loss at step 7800: 0.230770\n",
      "Minibatch learning rate at step 7800: 0.335131\n",
      "Minibatch loss at step 8000: 0.285640\n",
      "Minibatch learning rate at step 8000: 0.331710\n",
      "Minibatch loss at step 8200: 0.126701\n",
      "Minibatch learning rate at step 8200: 0.328325\n",
      "Minibatch loss at step 8400: 0.136554\n",
      "Minibatch learning rate at step 8400: 0.324974\n",
      "Minibatch loss at step 8600: 0.128964\n",
      "Minibatch learning rate at step 8600: 0.321657\n",
      "Minibatch loss at step 8800: 0.159443\n",
      "Minibatch learning rate at step 8800: 0.318374\n",
      "Minibatch loss at step 9000: 0.267681\n",
      "Minibatch learning rate at step 9000: 0.315125\n",
      "Minibatch loss at step 9200: 0.210409\n",
      "Minibatch learning rate at step 9200: 0.311908\n",
      "Minibatch loss at step 9400: 0.131124\n",
      "Minibatch learning rate at step 9400: 0.308725\n",
      "Minibatch loss at step 9600: 0.200466\n",
      "Minibatch learning rate at step 9600: 0.305574\n",
      "Minibatch loss at step 9800: 0.125697\n",
      "Minibatch learning rate at step 9800: 0.302455\n",
      "Minibatch loss at step 10000: 0.195817\n",
      "Minibatch learning rate at step 10000: 0.299368\n",
      "Minibatch loss at step 10200: 0.216664\n",
      "Minibatch learning rate at step 10200: 0.296313\n",
      "Minibatch loss at step 10400: 0.139492\n",
      "Minibatch learning rate at step 10400: 0.293289\n",
      "Minibatch loss at step 10600: 0.235726\n",
      "Minibatch learning rate at step 10600: 0.290295\n",
      "Minibatch loss at step 10800: 0.164746\n",
      "Minibatch learning rate at step 10800: 0.287333\n",
      "Minibatch loss at step 11000: 0.135968\n",
      "Minibatch learning rate at step 11000: 0.284400\n",
      "Minibatch loss at step 11200: 0.177753\n",
      "Minibatch learning rate at step 11200: 0.281497\n",
      "Minibatch loss at step 11400: 0.219234\n",
      "Minibatch learning rate at step 11400: 0.278624\n",
      "Minibatch loss at step 11600: 0.097827\n",
      "Minibatch learning rate at step 11600: 0.275781\n",
      "Minibatch loss at step 11800: 0.167752\n",
      "Minibatch learning rate at step 11800: 0.272966\n",
      "Minibatch loss at step 12000: 0.196175\n",
      "Minibatch learning rate at step 12000: 0.270180\n",
      "Minibatch loss at step 12200: 0.118060\n",
      "Minibatch learning rate at step 12200: 0.267422\n",
      "Minibatch loss at step 12400: 0.150677\n",
      "Minibatch learning rate at step 12400: 0.264693\n",
      "Minibatch loss at step 12600: 0.194430\n",
      "Minibatch learning rate at step 12600: 0.261992\n",
      "Minibatch loss at step 12800: 0.105766\n",
      "Minibatch learning rate at step 12800: 0.259318\n",
      "Minibatch loss at step 13000: 0.217609\n",
      "Minibatch learning rate at step 13000: 0.256671\n",
      "Minibatch loss at step 13200: 0.126505\n",
      "Minibatch learning rate at step 13200: 0.254051\n",
      "Minibatch loss at step 13400: 0.091349\n",
      "Minibatch learning rate at step 13400: 0.251458\n",
      "Minibatch loss at step 13600: 0.171283\n",
      "Minibatch learning rate at step 13600: 0.248892\n",
      "Minibatch loss at step 13800: 0.130222\n",
      "Minibatch learning rate at step 13800: 0.246352\n",
      "Minibatch loss at step 14000: 0.165320\n",
      "Minibatch learning rate at step 14000: 0.243837\n",
      "Minibatch loss at step 14200: 0.164047\n",
      "Minibatch learning rate at step 14200: 0.241349\n",
      "Minibatch loss at step 14400: 0.152921\n",
      "Minibatch learning rate at step 14400: 0.238886\n",
      "Minibatch loss at step 14600: 0.096958\n",
      "Minibatch learning rate at step 14600: 0.236447\n",
      "Minibatch loss at step 14800: 0.077777\n",
      "Minibatch learning rate at step 14800: 0.234034\n",
      "Minibatch loss at step 15000: 0.138479\n",
      "Minibatch learning rate at step 15000: 0.231646\n",
      "Minibatch loss at step 15200: 0.189880\n",
      "Minibatch learning rate at step 15200: 0.229281\n",
      "Minibatch loss at step 15400: 0.138896\n",
      "Minibatch learning rate at step 15400: 0.226941\n",
      "Minibatch loss at step 15600: 0.083301\n",
      "Minibatch learning rate at step 15600: 0.224625\n",
      "Minibatch loss at step 15800: 0.091426\n",
      "Minibatch learning rate at step 15800: 0.222332\n",
      "Minibatch loss at step 16000: 0.066033\n",
      "Minibatch learning rate at step 16000: 0.220063\n",
      "Minibatch loss at step 16200: 0.116350\n",
      "Minibatch learning rate at step 16200: 0.217817\n",
      "Minibatch loss at step 16400: 0.068467\n",
      "Minibatch learning rate at step 16400: 0.215594\n",
      "Minibatch loss at step 16600: 0.130658\n",
      "Minibatch learning rate at step 16600: 0.213394\n",
      "Minibatch loss at step 16800: 0.165271\n",
      "Minibatch learning rate at step 16800: 0.211216\n",
      "Minibatch loss at step 17000: 0.079125\n",
      "Minibatch learning rate at step 17000: 0.209060\n",
      "Minibatch loss at step 17200: 0.084339\n",
      "Minibatch learning rate at step 17200: 0.206926\n",
      "Minibatch loss at step 17400: 0.090396\n",
      "Minibatch learning rate at step 17400: 0.204814\n",
      "Minibatch loss at step 17600: 0.087892\n",
      "Minibatch learning rate at step 17600: 0.202724\n",
      "Minibatch loss at step 17800: 0.113728\n",
      "Minibatch learning rate at step 17800: 0.200655\n",
      "Minibatch loss at step 18000: 0.102903\n",
      "Minibatch learning rate at step 18000: 0.198607\n",
      "Minibatch loss at step 18200: 0.075280\n",
      "Minibatch learning rate at step 18200: 0.196580\n",
      "Minibatch loss at step 18400: 0.121954\n",
      "Minibatch learning rate at step 18400: 0.194574\n",
      "Minibatch loss at step 18600: 0.089441\n",
      "Minibatch learning rate at step 18600: 0.192588\n",
      "Minibatch loss at step 18800: 0.145493\n",
      "Minibatch learning rate at step 18800: 0.190622\n",
      "Minibatch loss at step 19000: 0.073780\n",
      "Minibatch learning rate at step 19000: 0.188677\n",
      "Minibatch loss at step 19200: 0.136417\n",
      "Minibatch learning rate at step 19200: 0.186751\n",
      "Minibatch loss at step 19400: 0.080288\n",
      "Minibatch learning rate at step 19400: 0.184845\n",
      "Minibatch loss at step 19600: 0.152420\n",
      "Minibatch learning rate at step 19600: 0.182958\n",
      "Minibatch loss at step 19800: 0.076397\n",
      "Minibatch learning rate at step 19800: 0.181091\n",
      "Minibatch loss at step 20000: 0.104721\n",
      "Minibatch learning rate at step 20000: 0.179243\n",
      "Minibatch loss at step 20200: 0.079829\n",
      "Minibatch learning rate at step 20200: 0.177414\n",
      "Minibatch loss at step 20400: 0.108212\n",
      "Minibatch learning rate at step 20400: 0.175603\n",
      "Minibatch loss at step 20600: 0.090225\n",
      "Minibatch learning rate at step 20600: 0.173811\n",
      "Minibatch loss at step 20800: 0.171867\n",
      "Minibatch learning rate at step 20800: 0.172037\n",
      "Minibatch loss at step 21000: 0.074214\n",
      "Minibatch learning rate at step 21000: 0.170281\n",
      "Minibatch loss at step 21200: 0.062265\n",
      "Minibatch learning rate at step 21200: 0.168543\n",
      "Minibatch loss at step 21400: 0.055876\n",
      "Minibatch learning rate at step 21400: 0.166823\n",
      "Minibatch loss at step 21600: 0.110607\n",
      "Minibatch learning rate at step 21600: 0.165120\n",
      "Minibatch loss at step 21800: 0.121575\n",
      "Minibatch learning rate at step 21800: 0.163435\n",
      "Minibatch loss at step 22000: 0.057631\n",
      "Minibatch learning rate at step 22000: 0.161767\n",
      "Minibatch loss at step 22200: 0.066685\n",
      "Minibatch learning rate at step 22200: 0.160116\n",
      "Minibatch loss at step 22400: 0.080792\n",
      "Minibatch learning rate at step 22400: 0.158482\n",
      "Minibatch loss at step 22600: 0.123322\n",
      "Minibatch learning rate at step 22600: 0.156864\n",
      "Minibatch loss at step 22800: 0.094224\n",
      "Minibatch learning rate at step 22800: 0.155263\n",
      "Minibatch loss at step 23000: 0.053433\n",
      "Minibatch learning rate at step 23000: 0.153678\n",
      "Minibatch loss at step 23200: 0.071099\n",
      "Minibatch learning rate at step 23200: 0.152110\n",
      "Minibatch loss at step 23400: 0.057646\n",
      "Minibatch learning rate at step 23400: 0.150557\n",
      "Minibatch loss at step 23600: 0.071432\n",
      "Minibatch learning rate at step 23600: 0.149021\n",
      "Minibatch loss at step 23800: 0.124355\n",
      "Minibatch learning rate at step 23800: 0.147500\n",
      "Minibatch loss at step 24000: 0.075311\n",
      "Minibatch learning rate at step 24000: 0.145994\n",
      "Minibatch loss at step 24200: 0.077175\n",
      "Minibatch learning rate at step 24200: 0.144504\n",
      "Minibatch loss at step 24400: 0.050915\n",
      "Minibatch learning rate at step 24400: 0.143030\n",
      "Minibatch loss at step 24600: 0.093833\n",
      "Minibatch learning rate at step 24600: 0.141570\n",
      "Minibatch loss at step 24800: 0.073760\n",
      "Minibatch learning rate at step 24800: 0.140125\n",
      "Minibatch loss at step 25000: 0.074722\n",
      "Minibatch learning rate at step 25000: 0.138695\n",
      "Minibatch loss at step 25200: 0.078178\n",
      "Minibatch learning rate at step 25200: 0.137279\n",
      "Minibatch loss at step 25400: 0.113063\n",
      "Minibatch learning rate at step 25400: 0.135878\n",
      "Minibatch loss at step 25600: 0.093531\n",
      "Minibatch learning rate at step 25600: 0.134491\n",
      "Minibatch loss at step 25800: 0.049682\n",
      "Minibatch learning rate at step 25800: 0.133119\n",
      "Minibatch loss at step 26000: 0.056491\n",
      "Minibatch learning rate at step 26000: 0.131760\n",
      "Minibatch loss at step 26200: 0.062405\n",
      "Minibatch learning rate at step 26200: 0.130415\n",
      "Minibatch loss at step 26400: 0.052534\n",
      "Minibatch learning rate at step 26400: 0.129084\n",
      "Minibatch loss at step 26600: 0.070903\n",
      "Minibatch learning rate at step 26600: 0.127767\n",
      "Minibatch loss at step 26800: 0.052550\n",
      "Minibatch learning rate at step 26800: 0.126463\n",
      "Minibatch loss at step 27000: 0.083479\n",
      "Minibatch learning rate at step 27000: 0.125172\n",
      "Minibatch loss at step 27200: 0.057702\n",
      "Minibatch learning rate at step 27200: 0.123894\n",
      "Minibatch loss at step 27400: 0.089528\n",
      "Minibatch learning rate at step 27400: 0.122630\n",
      "Minibatch loss at step 27600: 0.068012\n",
      "Minibatch learning rate at step 27600: 0.121378\n",
      "Minibatch loss at step 27800: 0.119166\n",
      "Minibatch learning rate at step 27800: 0.120140\n",
      "Minibatch loss at step 28000: 0.070348\n",
      "Minibatch learning rate at step 28000: 0.118913\n",
      "Minibatch loss at step 28200: 0.076977\n",
      "Minibatch learning rate at step 28200: 0.117700\n",
      "Minibatch loss at step 28400: 0.052270\n",
      "Minibatch learning rate at step 28400: 0.116498\n",
      "Minibatch loss at step 28600: 0.073300\n",
      "Minibatch learning rate at step 28600: 0.115309\n",
      "Minibatch loss at step 28800: 0.061668\n",
      "Minibatch learning rate at step 28800: 0.114133\n",
      "Minibatch loss at step 29000: 0.085386\n",
      "Minibatch learning rate at step 29000: 0.112968\n",
      "Minibatch loss at step 29200: 0.076249\n",
      "Minibatch learning rate at step 29200: 0.111815\n",
      "Minibatch loss at step 29400: 0.053442\n",
      "Minibatch learning rate at step 29400: 0.110674\n",
      "Minibatch loss at step 29600: 0.050573\n",
      "Minibatch learning rate at step 29600: 0.109544\n",
      "Minibatch loss at step 29800: 0.060712\n",
      "Minibatch learning rate at step 29800: 0.108426\n",
      "Minibatch loss at step 30000: 0.052999\n",
      "Minibatch learning rate at step 30000: 0.107319\n",
      "Final Test accuracy: 96.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 30001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  lossVec = []\n",
    "  trainAcc = []\n",
    "  validAcc = []\n",
    "  lrVec = []\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, lr, predictions = session.run([optimizer, loss, learning_rate, train_prediction], feed_dict=feed_dict)\n",
    "    lossVec.append(l)\n",
    "    lrVec.append(lr)\n",
    "    if (step % 200 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch learning rate at step %d: %f\" % (step, lr))\n",
    "      trainAcc.append(accuracy(predictions, batch_labels))\n",
    "      validAcc.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Final Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fName = 'Experiment_4.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(fName, 'wb')\n",
    "  save = {\n",
    "    'lossVec': lossVec,\n",
    "    'trainAcc': trainAcc,\n",
    "    'validAcc': validAcc,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'Experiment_4.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  lossVec = save['lossVec']\n",
    "  trainAcc = save['trainAcc']\n",
    "  validAcc = save['validAcc']\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEKCAYAAAA8QgPpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x144423450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEPCAYAAACHuClZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzrle887EOedcr/0/ZC4pIwWolNEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110126c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(lossVec)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1.5])\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xrange(0,30001,200),trainAcc, label= \"Minibatch Accuracy\")\n",
    "plt.plot(xrange(0,30001,200),validAcc, label= \"Valid Accuracy\")\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([70,100])\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc=4)\n",
    "#plt.savefig('Experiment_1.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
